{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, log_loss\n",
    "from bt_classes import my_backtest, test_indicator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "#importing required libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from sa import *\n",
    "from utils import *\n",
    "# follow the literature\n",
    "# we don't use min-max scaling here, use partial mean-std scaling instead\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import chain\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "# df = pd.read_csv('../res/input0130.csv')\n",
    "\n",
    "orig_df = pd.read_csv('../xau_1d_20y.csv')\n",
    "orig_df['datetime'] = pd.to_datetime(orig_df['date'])\n",
    "orig_df = orig_df.set_index('datetime')\n",
    "\n",
    "df = orig_df.copy()\n",
    "df['log_r'] = np.log(df['close']) - np.log(df['open'])\n",
    "df['label'] = np.sign(df['log_r'].shift(-1))\n",
    "df['label'][df['label']==-1] = 0\n",
    "df['label'] = df['label'].fillna(0)\n",
    "\n",
    "\n",
    "# Please select the last activation layer.\n",
    "layer_names = ['lstm_2']\n",
    "\n",
    "default_upper_bound = 2000\n",
    "default_n_bucket = 1000\n",
    "default_n_classes = 2\n",
    "class Args(): #创建一个类\n",
    "    def __init__(self): #定义初始化信息。\n",
    "        self.is_classification = True\n",
    "        self.save_path = ''\n",
    "        self.d = 'lstm_r'\n",
    "        self.num_classes = 2\n",
    "        self.lsa = True\n",
    "        self.dsa = True\n",
    "        self.target = 'none'\n",
    "        self.batch_size = 128\n",
    "        self.var_threshold = 1e-5\n",
    "        self.upper_bound = 2000\n",
    "        self.n_bucket = 1000\n",
    "        self.is_classification = True\n",
    "args = Args()\n",
    "\n",
    "def lstm_model(sample_len=240,para_a=42, para_b=17,drop1=0.05,drop2=0.02):\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(units=para_a, dropout=0.1, return_sequences=True, input_shape=(sample_len,1),activation='tanh'))# (25,15)-57, (42,17)-58\n",
    "    # model.add(LSTM(units=para_b, dropout=0.08, activation='tanh'))\n",
    "    model.add(CuDNNLSTM(units=para_a, return_sequences=True, input_shape=(sample_len,1)))# (25,15)-57, (42,17)-58\n",
    "    model.add(Dropout(drop1))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(CuDNNLSTM(units=para_b))\n",
    "    model.add(Dropout(drop2))\n",
    "    model.add(Activation('tanh'))\n",
    "    # model.add(Dropout(0.08))# 加了之后同原先效果差不多，（应该一定程度上）可以防止过拟合\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this experiment is intended for trying to calculate the transition probability matrix rollingly.\n",
    "# firstly let's define some useful functions\n",
    "def get_transtitions(y_true, y_pred):\n",
    "    '''\n",
    "    To generate transition probability matrix with y_true, y_pred of an any period.\n",
    "    '''\n",
    "    y_output = y_pred\n",
    "    y = y_true\n",
    "    continues_suc = 0\n",
    "    continues_fail = 0\n",
    "    result = []\n",
    "    maxx = 0\n",
    "    for i in range(0,len(y_output)):\n",
    "        if y_output[i] == y[i]:\n",
    "            continues_suc+=1\n",
    "            if continues_fail!=0:\n",
    "                result.append(-continues_fail)\n",
    "                if continues_fail > maxx:\n",
    "                    maxx = continues_fail\n",
    "                continues_fail = 0\n",
    "        else:\n",
    "            continues_fail+=1\n",
    "            if continues_suc != 0:\n",
    "                result.append(continues_suc)\n",
    "                if continues_suc > maxx:\n",
    "                    maxx = continues_suc\n",
    "                continues_suc = 0\n",
    "\n",
    "    length = maxx+1\n",
    "    suc_result = [[0] * length for i in range(length)]\n",
    "    fail_result = [[0]*length for i in range(length)]\n",
    "\n",
    "    for i in range(len(result)-1):\n",
    "        if result[i]>0:\n",
    "            suc_result[result[i]][-result[i+1]]+=1\n",
    "        else:\n",
    "            fail_result[-result[i]][result[i+1]]+=1\n",
    "    return suc_result, fail_result\n",
    "\n",
    "def get_trans_prob(suc_result, fail_result, weighted=False):\n",
    "    status_porb = {}\n",
    "    if weighted:\n",
    "        for i in range(len(suc_result)):\n",
    "            fail = np.sum([j*suc_result[i][j] for j in range(len(suc_result[i]))])\n",
    "            if i+1 < len(suc_result):\n",
    "                success = np.sum([(j-i)*np.sum(suc_result[j]) for j in range(i+1,len(suc_result))])\n",
    "                # success = np.sum(suc_result[i+1:])\n",
    "            else: \n",
    "                success = 0\n",
    "            status_porb[i] = success / (success + fail)\n",
    "\n",
    "        for i in range(len(fail_result)):\n",
    "            success = np.sum([j*fail_result[i][j] for j in range(len(fail_result[i]))])\n",
    "            if i+1 < len(fail_result):\n",
    "                fail = np.sum([(j-i)*np.sum(fail_result[j]) for j in range(i+1,len(fail_result))])\n",
    "                # fail = np.sum(fail_result[i+1:])\n",
    "            else: \n",
    "                fail = 0\n",
    "            status_porb[-i] = success / (success + fail)\n",
    "    else:\n",
    "        for i in range(len(suc_result)):\n",
    "            fail = np.sum(suc_result[i])\n",
    "            if i+1 < len(suc_result):\n",
    "                success = np.sum(suc_result[i+1:])\n",
    "            else: \n",
    "                success = 0\n",
    "            status_porb[i] = success / (success + fail)\n",
    "\n",
    "        for i in range(len(fail_result)):\n",
    "            success = np.sum(fail_result[i])\n",
    "            if i+1 < len(fail_result):\n",
    "                fail = np.sum(fail_result[i+1:])\n",
    "            else: \n",
    "                fail = 0\n",
    "            status_porb[-i] = success / (success + fail)\n",
    "    return status_porb\n",
    "\n",
    "def trans_prob(y_true, y_pred, weighted=False):\n",
    "    suc_result, fail_result = get_transtitions(y_true, y_pred)\n",
    "    return get_trans_prob(suc_result, fail_result, weighted)\n",
    "\n",
    "def get_suc_num(test_df):\n",
    "    test_df['win'] = -1\n",
    "    test_df['win'].loc[test_df['y_true']==test_df['y_pred']] = 1\n",
    "    test_df['suc_num'] = np.nan\n",
    "    test_df['suc_num'].loc[test_df['win']!=test_df['win'].shift(1)] = 1\n",
    "    test_df['suc_num'] = test_df['suc_num'].cumsum().fillna(method='ffill')\n",
    "    test_df['suc_num'] = test_df.groupby('suc_num')['suc_num'].cumsum() / test_df['suc_num'] * test_df['win']\n",
    "    return test_df['suc_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/30\n1500/1500 [==============================] - 0s 270us/step - loss: 0.6965 - accuracy: 0.4813\nEpoch 2/30\n1500/1500 [==============================] - 0s 47us/step - loss: 0.6932 - accuracy: 0.5047\nEpoch 3/30\n1500/1500 [==============================] - 0s 46us/step - loss: 0.6932 - accuracy: 0.5127\nEpoch 4/30\n1500/1500 [==============================] - 0s 44us/step - loss: 0.6931 - accuracy: 0.5080\nEpoch 5/30\n1500/1500 [==============================] - 0s 40us/step - loss: 0.6931 - accuracy: 0.5093\nEpoch 6/30\n1500/1500 [==============================] - 0s 40us/step - loss: 0.6927 - accuracy: 0.4993\nEpoch 7/30\n1500/1500 [==============================] - 0s 39us/step - loss: 0.6929 - accuracy: 0.5053\nEpoch 8/30\n1500/1500 [==============================] - 0s 36us/step - loss: 0.6929 - accuracy: 0.5147\nEpoch 9/30\n1500/1500 [==============================] - 0s 36us/step - loss: 0.6927 - accuracy: 0.5093\nEpoch 10/30\n1500/1500 [==============================] - 0s 37us/step - loss: 0.6927 - accuracy: 0.5147\nEpoch 11/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6928 - accuracy: 0.5133\nEpoch 12/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6930 - accuracy: 0.5073\nEpoch 13/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6924 - accuracy: 0.5113\nEpoch 14/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6924 - accuracy: 0.5147\nEpoch 15/30\n1500/1500 [==============================] - 0s 34us/step - loss: 0.6923 - accuracy: 0.5160\nEpoch 16/30\n1500/1500 [==============================] - 0s 36us/step - loss: 0.6927 - accuracy: 0.5033\nEpoch 17/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6928 - accuracy: 0.5060\nEpoch 18/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6925 - accuracy: 0.5173\nEpoch 19/30\n1500/1500 [==============================] - 0s 33us/step - loss: 0.6924 - accuracy: 0.5133\nEpoch 20/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6928 - accuracy: 0.5100\nEpoch 21/30\n1500/1500 [==============================] - 0s 34us/step - loss: 0.6920 - accuracy: 0.5273\nEpoch 22/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6921 - accuracy: 0.5087\nEpoch 23/30\n1500/1500 [==============================] - 0s 34us/step - loss: 0.6922 - accuracy: 0.5013\nEpoch 24/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6921 - accuracy: 0.5207\nEpoch 25/30\n1500/1500 [==============================] - 0s 34us/step - loss: 0.6924 - accuracy: 0.5240\nEpoch 26/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6915 - accuracy: 0.5247\nEpoch 27/30\n1500/1500 [==============================] - 0s 34us/step - loss: 0.6923 - accuracy: 0.4953\nEpoch 28/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6917 - accuracy: 0.5187\nEpoch 29/30\n1500/1500 [==============================] - 0s 34us/step - loss: 0.6919 - accuracy: 0.5187\nEpoch 30/30\n1500/1500 [==============================] - 0s 35us/step - loss: 0.6914 - accuracy: 0.5233\n"
    }
   ],
   "source": [
    "# reproduce training set\n",
    "sample_len = 9\n",
    "p1 = 192\n",
    "p2 = 192\n",
    "epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "train_begin = sample_len\n",
    "train_end = train_begin + 1500\n",
    "scaler = StandardScaler()\n",
    "train_set = df[['log_r','label']][train_begin-sample_len:train_end].reset_index()\n",
    "x_train, y_train = [], []\n",
    "x_train_set = list(chain.from_iterable(scaler.fit_transform(train_set['log_r'].values.reshape(-1,1))))\n",
    "for i in range(sample_len,len(x_train_set)):\n",
    "    x_train.append(x_train_set[i-sample_len:i])\n",
    "    y_train.append(train_set['label'][i])\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "y_train = to_categorical(y_train,2)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) \n",
    "\n",
    "model = lstm_model(sample_len=sample_len,para_a=p1,para_b=p2)\n",
    "model.fit(x_train,y_train,epochs=epochs, batch_size=batch_size, callbacks=[EarlyStopping(monitor='loss',patience=10)])\n",
    "model.save(f'd{sample_len}-{p1}_{p2}_{epochs}_{batch_size}.h5')\n",
    "\n",
    "# from keras.models import load_model\n",
    "# model = load_model(f'd{sample_len}-{p1}_{p2}_{epochs}_{batch_size}.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Before adjusted:\nAccuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 93810.00\nSharpe: -1.36\nMax drawdown: 7.63%\nAnnual rate: -14.87%\n\nAfter adjusted:\nAdjusted accuracy: 0.6400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 104846.00\nSharpe: 0.99\nMax drawdown: 7.58%\nAnnual rate: 12.67%\n\nBefore adjusted:\nAccuracy: 0.5000\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 89705.00\nSharpe: -1.52\nMax drawdown: 17.60%\nAnnual rate: -23.95%\n\nAfter adjusted:\nAdjusted accuracy: 0.4300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 102754.00\nSharpe: 0.49\nMax drawdown: 6.27%\nAnnual rate: 7.09%\n\nBefore adjusted:\nAccuracy: 0.6300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 113170.00\nSharpe: 3.19\nMax drawdown: 1.74%\nAnnual rate: 36.58%\n\nAfter adjusted:\nAdjusted accuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 93999.00\nSharpe: -1.59\nMax drawdown: 8.41%\nAnnual rate: -14.44%\n\nBefore adjusted:\nAccuracy: 0.4800\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 99479.00\nSharpe: -0.21\nMax drawdown: 5.66%\nAnnual rate: -1.31%\n\nAfter adjusted:\nAdjusted accuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 99479.00\nSharpe: -0.21\nMax drawdown: 5.66%\nAnnual rate: -1.31%\n\nBefore adjusted:\nAccuracy: 0.5600\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 105464.00\nSharpe: 1.56\nMax drawdown: 2.69%\nAnnual rate: 14.35%\n\nAfter adjusted:\nAdjusted accuracy: 0.4800\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 87856.00\nSharpe: -4.45\nMax drawdown: 13.52%\nAnnual rate: -27.84%\n\nBefore adjusted:\nAccuracy: 0.4500\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 95406.00\nSharpe: -0.66\nMax drawdown: 12.77%\nAnnual rate: -11.18%\n\nAfter adjusted:\nAdjusted accuracy: 0.4500\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 90411.00\nSharpe: -1.45\nMax drawdown: 14.95%\nAnnual rate: -22.43%\n\nBefore adjusted:\nAccuracy: 0.5800\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 107088.00\nSharpe: 0.98\nMax drawdown: 5.75%\nAnnual rate: 18.84%\n\nAfter adjusted:\nAdjusted accuracy: 0.3400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 73846.00\nSharpe: -3.36\nMax drawdown: 28.10%\nAnnual rate: -53.42%\n\nBefore adjusted:\nAccuracy: 0.5300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 97490.00\nSharpe: -0.11\nMax drawdown: 13.69%\nAnnual rate: -6.21%\n\nAfter adjusted:\nAdjusted accuracy: 0.5100\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 112712.00\nSharpe: 1.22\nMax drawdown: 13.82%\nAnnual rate: 35.20%\n\nBefore adjusted:\nAccuracy: 0.4900\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 97748.00\nSharpe: -0.16\nMax drawdown: 11.17%\nAnnual rate: -5.58%\n\nAfter adjusted:\nAdjusted accuracy: 0.5300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 103113.00\nSharpe: 0.41\nMax drawdown: 10.96%\nAnnual rate: 8.03%\n\nBefore adjusted:\nAccuracy: 0.5200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 98143.00\nSharpe: -0.34\nMax drawdown: 9.23%\nAnnual rate: -4.61%\n\nAfter adjusted:\nAdjusted accuracy: 0.5700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 101076.00\nSharpe: 0.20\nMax drawdown: 6.25%\nAnnual rate: 2.73%\n\nBefore adjusted:\nAccuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 93374.00\nSharpe: -0.89\nMax drawdown: 14.80%\nAnnual rate: -15.87%\n\nAfter adjusted:\nAdjusted accuracy: 0.5600\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 99735.00\nSharpe: -0.03\nMax drawdown: 8.96%\nAnnual rate: -0.67%\n\nBefore adjusted:\nAccuracy: 0.4500\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 110045.00\nSharpe: 1.35\nMax drawdown: 8.77%\nAnnual rate: 27.28%\n\nAfter adjusted:\nAdjusted accuracy: 0.4900\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 89718.00\nSharpe: -1.71\nMax drawdown: 16.74%\nAnnual rate: -23.92%\n\nBefore adjusted:\nAccuracy: 0.4400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 91020.00\nSharpe: -1.55\nMax drawdown: 14.02%\nAnnual rate: -21.11%\n\nAfter adjusted:\nAdjusted accuracy: 0.5500\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 108037.00\nSharpe: 1.31\nMax drawdown: 5.97%\nAnnual rate: 21.51%\n\nBefore adjusted:\nAccuracy: 0.6100\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 120462.00\nSharpe: 2.54\nMax drawdown: 6.07%\nAnnual rate: 59.86%\n\nAfter adjusted:\nAdjusted accuracy: 0.3400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 76139.00\nSharpe: -3.06\nMax drawdown: 27.51%\nAnnual rate: -49.69%\n\nBefore adjusted:\nAccuracy: 0.5800\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 102992.00\nSharpe: 0.45\nMax drawdown: 9.42%\nAnnual rate: 7.71%\n\nAfter adjusted:\nAdjusted accuracy: 0.6000\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 109094.00\nSharpe: 1.35\nMax drawdown: 6.67%\nAnnual rate: 24.53%\n\nBefore adjusted:\nAccuracy: 0.4900\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 68832.00\nSharpe: -1.23\nMax drawdown: 50.05%\nAnnual rate: -60.98%\n\nAfter adjusted:\nAdjusted accuracy: 0.5200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 81608.00\nSharpe: -0.59\nMax drawdown: 49.94%\nAnnual rate: -40.08%\n\nBefore adjusted:\nAccuracy: 0.4500\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 112172.00\nSharpe: 1.16\nMax drawdown: 9.83%\nAnnual rate: 33.57%\n\nAfter adjusted:\nAdjusted accuracy: 0.5200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 97872.00\nSharpe: -0.16\nMax drawdown: 15.53%\nAnnual rate: -5.28%\n\nBefore adjusted:\nAccuracy: 0.5500\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 117470.00\nSharpe: 1.89\nMax drawdown: 10.26%\nAnnual rate: 50.04%\n\nAfter adjusted:\nAdjusted accuracy: 0.5200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 85764.00\nSharpe: -1.64\nMax drawdown: 24.12%\nAnnual rate: -32.09%\n\nBefore adjusted:\nAccuracy: 0.5300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 98832.00\nSharpe: -0.13\nMax drawdown: 9.47%\nAnnual rate: -2.92%\n\nAfter adjusted:\nAdjusted accuracy: 0.4600\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 89466.00\nSharpe: -1.55\nMax drawdown: 17.02%\nAnnual rate: -24.46%\n\nBefore adjusted:\nAccuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 90368.00\nSharpe: -0.66\nMax drawdown: 17.83%\nAnnual rate: -22.53%\n\nAfter adjusted:\nAdjusted accuracy: 0.4800\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 57757.00\nSharpe: -3.15\nMax drawdown: 45.51%\nAnnual rate: -74.92%\n\nBefore adjusted:\nAccuracy: 0.4600\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 69998.00\nSharpe: -2.58\nMax drawdown: 36.29%\nAnnual rate: -59.30%\n\nAfter adjusted:\nAdjusted accuracy: 0.4800\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 98198.00\nSharpe: -0.10\nMax drawdown: 12.68%\nAnnual rate: -4.48%\n\nBefore adjusted:\nAccuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 90984.00\nSharpe: -1.18\nMax drawdown: 12.07%\nAnnual rate: -21.19%\n\nAfter adjusted:\nAdjusted accuracy: 0.4400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 87209.00\nSharpe: -2.02\nMax drawdown: 16.73%\nAnnual rate: -29.17%\n\nBefore adjusted:\nAccuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 90079.00\nSharpe: -1.68\nMax drawdown: 13.33%\nAnnual rate: -23.15%\n\nAfter adjusted:\nAdjusted accuracy: 0.4400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 89393.00\nSharpe: -1.91\nMax drawdown: 14.70%\nAnnual rate: -24.62%\n\nBefore adjusted:\nAccuracy: 0.4100\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 75709.00\nSharpe: -3.15\nMax drawdown: 25.34%\nAnnual rate: -50.40%\n\nAfter adjusted:\nAdjusted accuracy: 0.5400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 96535.00\nSharpe: -0.48\nMax drawdown: 11.26%\nAnnual rate: -8.50%\n\nBefore adjusted:\nAccuracy: 0.4500\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 85597.00\nSharpe: -2.06\nMax drawdown: 19.35%\nAnnual rate: -32.42%\n\nAfter adjusted:\nAdjusted accuracy: 0.5200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 106388.00\nSharpe: 1.02\nMax drawdown: 6.96%\nAnnual rate: 16.89%\n\nBefore adjusted:\nAccuracy: 0.5600\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 91543.00\nSharpe: -1.62\nMax drawdown: 12.85%\nAnnual rate: -19.96%\n\nAfter adjusted:\nAdjusted accuracy: 0.4400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 99194.00\nSharpe: -0.19\nMax drawdown: 4.12%\nAnnual rate: -2.02%\n\nBefore adjusted:\nAccuracy: 0.4200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 88962.00\nSharpe: -1.52\nMax drawdown: 14.90%\nAnnual rate: -25.53%\n\nAfter adjusted:\nAdjusted accuracy: 0.5100\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 97052.00\nSharpe: -0.41\nMax drawdown: 6.02%\nAnnual rate: -7.26%\n\nBefore adjusted:\nAccuracy: 0.4900\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 93026.00\nSharpe: -0.84\nMax drawdown: 12.88%\nAnnual rate: -16.65%\n\nAfter adjusted:\nAdjusted accuracy: 0.5000\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 101818.00\nSharpe: 0.27\nMax drawdown: 13.55%\nAnnual rate: 4.64%\n\nBefore adjusted:\nAccuracy: 0.5300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 105515.00\nSharpe: 0.94\nMax drawdown: 5.95%\nAnnual rate: 14.49%\n\nAfter adjusted:\nAdjusted accuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 105672.00\nSharpe: 1.11\nMax drawdown: 5.90%\nAnnual rate: 14.92%\n\nBefore adjusted:\nAccuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 100600.00\nSharpe: 0.10\nMax drawdown: 9.08%\nAnnual rate: 1.52%\n\nAfter adjusted:\nAdjusted accuracy: 0.4100\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 96050.00\nSharpe: -0.81\nMax drawdown: 12.19%\nAnnual rate: -9.66%\n\nBefore adjusted:\nAccuracy: 0.5300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 101972.00\nSharpe: 0.37\nMax drawdown: 5.83%\nAnnual rate: 5.04%\n\nAfter adjusted:\nAdjusted accuracy: 0.4800\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 87490.00\nSharpe: -2.87\nMax drawdown: 15.32%\nAnnual rate: -28.59%\n\nBefore adjusted:\nAccuracy: 0.5000\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 87730.00\nSharpe: -2.57\nMax drawdown: 13.15%\nAnnual rate: -28.10%\n\nAfter adjusted:\nAdjusted accuracy: 0.5200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 89816.00\nSharpe: -2.11\nMax drawdown: 13.41%\nAnnual rate: -23.71%\n\nBefore adjusted:\nAccuracy: 0.5200\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 94907.00\nSharpe: -1.05\nMax drawdown: 8.12%\nAnnual rate: -12.34%\n\nAfter adjusted:\nAdjusted accuracy: 0.4700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 99490.00\nSharpe: -0.15\nMax drawdown: 5.15%\nAnnual rate: -1.28%\n\nBefore adjusted:\nAccuracy: 0.4100\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 87149.00\nSharpe: -3.13\nMax drawdown: 14.60%\nAnnual rate: -29.29%\n\nAfter adjusted:\nAdjusted accuracy: 0.5700\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 97426.00\nSharpe: -0.79\nMax drawdown: 5.27%\nAnnual rate: -6.36%\n\nBefore adjusted:\nAccuracy: 0.5000\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 108334.00\nSharpe: 2.08\nMax drawdown: 4.73%\nAnnual rate: 22.35%\n\nAfter adjusted:\nAdjusted accuracy: 0.4900\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 90950.00\nSharpe: -2.25\nMax drawdown: 12.56%\nAnnual rate: -21.26%\n\nBefore adjusted:\nAccuracy: 0.5000\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 114775.00\nSharpe: 2.41\nMax drawdown: 8.52%\nAnnual rate: 41.52%\n\nAfter adjusted:\nAdjusted accuracy: 0.4900\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 97197.00\nSharpe: -0.50\nMax drawdown: 15.31%\nAnnual rate: -6.91%\n\nBefore adjusted:\nAccuracy: 0.5400\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 106520.00\nSharpe: 1.03\nMax drawdown: 9.03%\nAnnual rate: 17.25%\n\nAfter adjusted:\nAdjusted accuracy: 0.5300\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 105423.00\nSharpe: 1.05\nMax drawdown: 5.77%\nAnnual rate: 14.23%\n"
    }
   ],
   "source": [
    "## Main experiment 1:\n",
    "# first get first window test set and rolling\n",
    "prob_save = []\n",
    "window = 100\n",
    "starters = range(train_end,len(df)-window,window)\n",
    "all_result = []\n",
    "for test_begin in starters:\n",
    "    test_end = test_begin + window\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    test_set = df[['log_r','label']][test_begin-sample_len:test_end].reset_index()\n",
    "    test_df = df[test_begin:test_end].fillna(0)\n",
    "    x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(sample_len,len(x_test_set)):\n",
    "        x_test.append(x_test_set[i-sample_len:i])\n",
    "        y_test.append(test_set['label'][i-1])\n",
    "    test_df['y_true'] = y_test\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "    y_test = to_categorical(y_test,2)\n",
    "\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    test_df['y_pred'] = y_pred\n",
    "    y_true = test_df['y_true'].values\n",
    "    if test_begin == train_end:\n",
    "        ti = test_indicator(test_df)\n",
    "        # last_prob = ti.get_status_win_rate()\n",
    "        suc_fail,fail_suc = get_transtitions(y_true,y_pred)\n",
    "        last_prob = get_trans_prob(suc_fail,fail_suc,True)\n",
    "        print(\"Before adjusted:\")\n",
    "        ti.backtest()\n",
    "        ti.status_prob = last_prob\n",
    "        print(\"\\nAfter adjusted:\")\n",
    "        ti.backtest(prob_adjusted=True)\n",
    "        prob_save.append(last_prob)\n",
    "        continue\n",
    "    this_result = [test_begin]\n",
    "    ti2 = test_indicator(test_df)\n",
    "    print(\"\\nBefore adjusted:\")\n",
    "    this_result += list(ti2.backtest())\n",
    "    print(\"\\nAfter adjusted:\")\n",
    "    suc_fail,fail_suc = get_transtitions(y_true,y_pred)\n",
    "    ti2.status_prob, last_prob = last_prob, get_trans_prob(suc_fail,fail_suc,True)\n",
    "    this_result+= list(ti2.backtest(prob_adjusted=True))\n",
    "    all_result.append(this_result)\n",
    "    prob_save.append(last_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": " 0    0.000000\n 1    0.140070\n 2    0.169174\n 3    0.276323\n 4    0.392996\n 5    0.437662\n 6    0.490573\n 7    0.527046\n-1    0.143245\n-2    0.180405\n-3    0.248240\n-4    0.353075\n-5    0.450882\n-6    0.489864\n-7    0.456269\n 8    0.547723\n 9    0.000000\n-8    0.500000\n-9    0.000000\ndtype: float64"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "prob = pd.DataFrame(prob_save)\n",
    "prob.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0         1         2         3         4         5     6    7  \\\n0   0.0  0.352941  0.333333  0.375000  1.000000  1.000000  0.00  NaN   \n1   0.0  0.454545  0.590909  0.545455  0.400000  0.000000   NaN  NaN   \n2   0.0  0.711538  0.685714  0.944444  0.785714  0.600000  1.00  1.0   \n3   0.0  0.392857  0.650000  0.636364  0.666667  1.000000  0.00  NaN   \n4   0.0  0.520000  0.545455  0.454545  0.400000  1.000000  0.00  NaN   \n5   0.0  0.420000  0.322581  0.750000  0.600000  1.000000  1.00  0.0   \n6   0.0  0.620000  0.842105  0.190476  1.000000  0.000000   NaN  NaN   \n7   0.0  0.538462  0.809524  0.529412  0.714286  1.000000  0.20  0.0   \n8   0.0  0.600000  0.281250  0.625000  1.000000  0.250000  0.00  NaN   \n9   0.0  0.636364  0.451613  0.777778  0.222222  0.000000   NaN  NaN   \n10  0.0  0.647059  0.242424  0.333333  0.000000       NaN   NaN  NaN   \n11  0.0  0.313725  0.300000  0.400000  0.000000       NaN   NaN  NaN   \n12  0.0  0.320755  0.347826  1.000000  1.000000  1.000000  0.00  NaN   \n13  0.0  0.750000  0.777778  0.409091  0.500000  0.666667  1.00  0.0   \n14  0.0  0.688889  0.680000  0.473684  1.000000  1.000000  0.75  1.0   \n15  0.0  0.320755  0.500000  0.750000  1.000000  1.000000  1.00  1.0   \n16  0.0  0.360000  0.454545  0.461538  0.800000  1.000000  1.00  1.0   \n17  0.0  0.519231  0.538462  0.727273  0.833333  0.750000  1.00  1.0   \n18  0.0  0.448980  0.473684  0.200000  1.000000  0.000000   NaN  NaN   \n19  0.0  0.465116  0.347826  0.272727  0.000000       NaN   NaN  NaN   \n20  0.0  0.377358  0.590909  1.000000  0.555556  0.400000  0.00  NaN   \n21  0.0  0.511628  0.320000  0.300000  0.200000  0.000000   NaN  NaN   \n22  0.0  0.313725  0.500000  0.000000       NaN       NaN   NaN  NaN   \n23  0.0  0.226415  0.285714  1.000000  0.000000       NaN   NaN  NaN   \n24  0.0  0.294118  0.428571  0.200000  1.000000  0.000000   NaN  NaN   \n25  0.0  0.565217  0.619048  0.312500  0.666667  0.000000   NaN  NaN   \n26  0.0  0.369565  0.192308  0.166667  0.000000       NaN   NaN  NaN   \n27  0.0  0.423077  0.523810  0.444444  0.166667  0.000000   NaN  NaN   \n28  0.0  0.562500  0.636364  0.727273  0.200000  0.500000  0.00  NaN   \n29  0.0  0.294118  0.428571  0.142857  0.000000       NaN   NaN  NaN   \n30  0.0  0.531915  0.521739  0.294118  1.000000  0.500000  0.00  NaN   \n31  0.0  0.560976  0.285714  0.444444  1.000000  0.000000   NaN  NaN   \n32  0.0  0.388889  0.611111  0.625000  0.250000  0.000000   NaN  NaN   \n33  0.0  0.222222  0.250000  0.000000       NaN       NaN   NaN  NaN   \n34  0.0  0.431373  0.476190  0.857143  0.428571  1.000000  1.00  0.0   \n35  0.0  0.489362  0.423077  0.454545  1.000000  0.500000  0.00  NaN   \n36  0.0  0.675000  0.285714  0.250000  0.000000       NaN   NaN  NaN   \n\n          -1        -2        -3        -4        -5    -6   -7    8    9  \\\n0   0.500000  0.285714  0.384615  0.666667  0.000000  0.00  1.0  NaN  NaN   \n1   0.456522  0.666667  0.300000  0.166667  0.000000  0.00  0.0  NaN  NaN   \n2   0.769231  0.785714  1.000000       NaN       NaN   NaN  NaN  0.0  NaN   \n3   0.446809  0.500000  0.642857  0.600000  0.000000  1.00  NaN  NaN  NaN   \n4   0.722222  0.600000  0.571429  0.000000  0.000000  1.00  NaN  NaN  NaN   \n5   0.384615  0.321429  0.400000  0.111111  0.285714  0.00  0.5  NaN  NaN   \n6   0.685185  0.529412  0.500000  0.666667  1.000000   NaN  NaN  NaN  NaN   \n7   0.620000  0.550000  0.333333  0.750000  1.000000   NaN  NaN  NaN  NaN   \n8   0.480000  0.520000  0.444444  1.000000       NaN   NaN  NaN  NaN  NaN   \n9   0.418605  0.607143  0.500000  0.857143  1.000000   NaN  NaN  NaN  NaN   \n10  0.437500  0.400000  0.428571  0.500000  1.000000   NaN  NaN  NaN  NaN   \n11  0.428571  0.423077  0.272727  0.428571  0.000000  1.00  NaN  NaN  NaN   \n12  0.325581  0.347826  0.650000  0.333333  0.000000  0.00  0.0  NaN  NaN   \n13  0.690476  0.903226  0.666667  1.000000       NaN   NaN  NaN  NaN  NaN   \n14  0.693878  0.869565  1.000000       NaN       NaN   NaN  NaN  1.0  0.0   \n15  0.636364  0.500000  0.166667  0.333333  1.000000   NaN  NaN  1.0  0.0   \n16  0.325581  0.464286  0.461538  0.769231  0.500000  1.00  NaN  0.0  NaN   \n17  0.721311  0.090909  0.333333  1.000000       NaN   NaN  NaN  0.0  NaN   \n18  0.688889  0.812500  0.750000  1.000000       NaN   NaN  NaN  NaN  NaN   \n19  0.480769  0.517241  0.333333  0.000000  0.000000  0.00  1.0  NaN  NaN   \n20  0.138889  0.558824  0.750000  1.000000       NaN   NaN  NaN  NaN  NaN   \n21  0.311111  0.379310  0.375000  0.666667  0.500000  0.00  1.0  NaN  NaN   \n22  0.488889  0.625000  0.000000  0.500000  0.750000  1.00  NaN  NaN  NaN   \n23  0.382979  0.480000  0.454545  0.000000  1.000000   NaN  NaN  NaN  NaN   \n24  0.409091  0.620690  0.285714  0.800000  1.000000   NaN  NaN  NaN  NaN   \n25  0.645833  0.500000  0.727273  0.000000  0.000000  1.00  NaN  NaN  NaN   \n26  0.413793  0.107143  0.136364  0.176471  0.000000  0.25  0.4  NaN  NaN   \n27  0.553191  0.454545  0.272727  0.500000  0.000000  0.50  1.0  NaN  NaN   \n28  0.604167  0.692308  0.333333  0.500000  0.000000  1.00  NaN  NaN  NaN   \n29  0.325000  0.696970  0.666667  0.750000  1.000000   NaN  NaN  NaN  NaN   \n30  0.512195  0.653846  0.444444  0.000000  0.857143  1.00  NaN  NaN  NaN   \n31  0.540000  0.521739  0.000000  0.750000  0.000000  0.00  1.0  NaN  NaN   \n32  0.617021  0.588235  0.769231  0.000000  0.000000  1.00  NaN  NaN  NaN   \n33  0.461538  0.347826  0.333333  0.375000  0.000000  0.00  0.0  NaN  NaN   \n34  0.557692  0.333333  0.200000  0.666667  0.333333  0.00  1.0  NaN  NaN   \n35  0.560000  0.437500  0.818182  1.000000       NaN   NaN  NaN  NaN  NaN   \n36  0.640000  0.529412  0.777778  1.000000       NaN   NaN  NaN  NaN  NaN   \n\n     -8   -9  \n0   NaN  NaN  \n1   0.0  1.0  \n2   NaN  NaN  \n3   NaN  NaN  \n4   NaN  NaN  \n5   1.0  NaN  \n6   NaN  NaN  \n7   NaN  NaN  \n8   NaN  NaN  \n9   NaN  NaN  \n10  NaN  NaN  \n11  NaN  NaN  \n12  1.0  NaN  \n13  NaN  NaN  \n14  NaN  NaN  \n15  NaN  NaN  \n16  NaN  NaN  \n17  NaN  NaN  \n18  NaN  NaN  \n19  NaN  NaN  \n20  NaN  NaN  \n21  NaN  NaN  \n22  NaN  NaN  \n23  NaN  NaN  \n24  NaN  NaN  \n25  NaN  NaN  \n26  0.5  1.0  \n27  NaN  NaN  \n28  NaN  NaN  \n29  NaN  NaN  \n30  NaN  NaN  \n31  NaN  NaN  \n32  NaN  NaN  \n33  0.0  1.0  \n34  NaN  NaN  \n35  NaN  NaN  \n36  NaN  NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>-1</th>\n      <th>-2</th>\n      <th>-3</th>\n      <th>-4</th>\n      <th>-5</th>\n      <th>-6</th>\n      <th>-7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>-8</th>\n      <th>-9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.352941</td>\n      <td>0.333333</td>\n      <td>0.375000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.500000</td>\n      <td>0.285714</td>\n      <td>0.384615</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.454545</td>\n      <td>0.590909</td>\n      <td>0.545455</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.456522</td>\n      <td>0.666667</td>\n      <td>0.300000</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.711538</td>\n      <td>0.685714</td>\n      <td>0.944444</td>\n      <td>0.785714</td>\n      <td>0.600000</td>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>0.769231</td>\n      <td>0.785714</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.392857</td>\n      <td>0.650000</td>\n      <td>0.636364</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.446809</td>\n      <td>0.500000</td>\n      <td>0.642857</td>\n      <td>0.600000</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.520000</td>\n      <td>0.545455</td>\n      <td>0.454545</td>\n      <td>0.400000</td>\n      <td>1.000000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.722222</td>\n      <td>0.600000</td>\n      <td>0.571429</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.420000</td>\n      <td>0.322581</td>\n      <td>0.750000</td>\n      <td>0.600000</td>\n      <td>1.000000</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>0.384615</td>\n      <td>0.321429</td>\n      <td>0.400000</td>\n      <td>0.111111</td>\n      <td>0.285714</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0</td>\n      <td>0.620000</td>\n      <td>0.842105</td>\n      <td>0.190476</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.685185</td>\n      <td>0.529412</td>\n      <td>0.500000</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0</td>\n      <td>0.538462</td>\n      <td>0.809524</td>\n      <td>0.529412</td>\n      <td>0.714286</td>\n      <td>1.000000</td>\n      <td>0.20</td>\n      <td>0.0</td>\n      <td>0.620000</td>\n      <td>0.550000</td>\n      <td>0.333333</td>\n      <td>0.750000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0</td>\n      <td>0.600000</td>\n      <td>0.281250</td>\n      <td>0.625000</td>\n      <td>1.000000</td>\n      <td>0.250000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.480000</td>\n      <td>0.520000</td>\n      <td>0.444444</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0</td>\n      <td>0.636364</td>\n      <td>0.451613</td>\n      <td>0.777778</td>\n      <td>0.222222</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.418605</td>\n      <td>0.607143</td>\n      <td>0.500000</td>\n      <td>0.857143</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.0</td>\n      <td>0.647059</td>\n      <td>0.242424</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.437500</td>\n      <td>0.400000</td>\n      <td>0.428571</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.0</td>\n      <td>0.313725</td>\n      <td>0.300000</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.428571</td>\n      <td>0.423077</td>\n      <td>0.272727</td>\n      <td>0.428571</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.0</td>\n      <td>0.320755</td>\n      <td>0.347826</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.325581</td>\n      <td>0.347826</td>\n      <td>0.650000</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.0</td>\n      <td>0.750000</td>\n      <td>0.777778</td>\n      <td>0.409091</td>\n      <td>0.500000</td>\n      <td>0.666667</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>0.690476</td>\n      <td>0.903226</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.0</td>\n      <td>0.688889</td>\n      <td>0.680000</td>\n      <td>0.473684</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.75</td>\n      <td>1.0</td>\n      <td>0.693878</td>\n      <td>0.869565</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.0</td>\n      <td>0.320755</td>\n      <td>0.500000</td>\n      <td>0.750000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>0.636364</td>\n      <td>0.500000</td>\n      <td>0.166667</td>\n      <td>0.333333</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.0</td>\n      <td>0.360000</td>\n      <td>0.454545</td>\n      <td>0.461538</td>\n      <td>0.800000</td>\n      <td>1.000000</td>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>0.325581</td>\n      <td>0.464286</td>\n      <td>0.461538</td>\n      <td>0.769231</td>\n      <td>0.500000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.0</td>\n      <td>0.519231</td>\n      <td>0.538462</td>\n      <td>0.727273</td>\n      <td>0.833333</td>\n      <td>0.750000</td>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>0.721311</td>\n      <td>0.090909</td>\n      <td>0.333333</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.0</td>\n      <td>0.448980</td>\n      <td>0.473684</td>\n      <td>0.200000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.688889</td>\n      <td>0.812500</td>\n      <td>0.750000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.0</td>\n      <td>0.465116</td>\n      <td>0.347826</td>\n      <td>0.272727</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.480769</td>\n      <td>0.517241</td>\n      <td>0.333333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.0</td>\n      <td>0.377358</td>\n      <td>0.590909</td>\n      <td>1.000000</td>\n      <td>0.555556</td>\n      <td>0.400000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.138889</td>\n      <td>0.558824</td>\n      <td>0.750000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.0</td>\n      <td>0.511628</td>\n      <td>0.320000</td>\n      <td>0.300000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.311111</td>\n      <td>0.379310</td>\n      <td>0.375000</td>\n      <td>0.666667</td>\n      <td>0.500000</td>\n      <td>0.00</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.0</td>\n      <td>0.313725</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.488889</td>\n      <td>0.625000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.750000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.0</td>\n      <td>0.226415</td>\n      <td>0.285714</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.382979</td>\n      <td>0.480000</td>\n      <td>0.454545</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.0</td>\n      <td>0.294118</td>\n      <td>0.428571</td>\n      <td>0.200000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.409091</td>\n      <td>0.620690</td>\n      <td>0.285714</td>\n      <td>0.800000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.0</td>\n      <td>0.565217</td>\n      <td>0.619048</td>\n      <td>0.312500</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.645833</td>\n      <td>0.500000</td>\n      <td>0.727273</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.0</td>\n      <td>0.369565</td>\n      <td>0.192308</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.413793</td>\n      <td>0.107143</td>\n      <td>0.136364</td>\n      <td>0.176471</td>\n      <td>0.000000</td>\n      <td>0.25</td>\n      <td>0.4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.5</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.0</td>\n      <td>0.423077</td>\n      <td>0.523810</td>\n      <td>0.444444</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.553191</td>\n      <td>0.454545</td>\n      <td>0.272727</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.50</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.0</td>\n      <td>0.562500</td>\n      <td>0.636364</td>\n      <td>0.727273</td>\n      <td>0.200000</td>\n      <td>0.500000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.604167</td>\n      <td>0.692308</td>\n      <td>0.333333</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.0</td>\n      <td>0.294118</td>\n      <td>0.428571</td>\n      <td>0.142857</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.325000</td>\n      <td>0.696970</td>\n      <td>0.666667</td>\n      <td>0.750000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.0</td>\n      <td>0.531915</td>\n      <td>0.521739</td>\n      <td>0.294118</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.512195</td>\n      <td>0.653846</td>\n      <td>0.444444</td>\n      <td>0.000000</td>\n      <td>0.857143</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.0</td>\n      <td>0.560976</td>\n      <td>0.285714</td>\n      <td>0.444444</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.540000</td>\n      <td>0.521739</td>\n      <td>0.000000</td>\n      <td>0.750000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.0</td>\n      <td>0.388889</td>\n      <td>0.611111</td>\n      <td>0.625000</td>\n      <td>0.250000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.617021</td>\n      <td>0.588235</td>\n      <td>0.769231</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.0</td>\n      <td>0.222222</td>\n      <td>0.250000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.461538</td>\n      <td>0.347826</td>\n      <td>0.333333</td>\n      <td>0.375000</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.0</td>\n      <td>0.431373</td>\n      <td>0.476190</td>\n      <td>0.857143</td>\n      <td>0.428571</td>\n      <td>1.000000</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>0.557692</td>\n      <td>0.333333</td>\n      <td>0.200000</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.00</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.0</td>\n      <td>0.489362</td>\n      <td>0.423077</td>\n      <td>0.454545</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>0.560000</td>\n      <td>0.437500</td>\n      <td>0.818182</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.0</td>\n      <td>0.675000</td>\n      <td>0.285714</td>\n      <td>0.250000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.640000</td>\n      <td>0.529412</td>\n      <td>0.777778</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "starts      3359.000000\npre-acc        0.500278\nsharpe        -0.244191\ndrawdown      12.689832\nannual        -4.004730\nadj-acc        0.488611\nsharpe        -0.808469\ndrawdown      13.967464\nannual       -10.961667\ndtype: float64"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "a = pd.DataFrame(all_result,columns=['starts','pre-acc','sharpe','drawdown','annual','adj-acc','sharpe','drawdown','annual'])\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 滚动的测试：先把全部预测、连续对错状态都算出来，再遍历判断修改\n",
    "test_len = 500\n",
    "weighted = False\n",
    "test_begin = train_end\n",
    "test_end = test_begin + test_len\n",
    "\n",
    "x_test, y_test = [], []\n",
    "test_set = df[['log_r','label']][test_begin-sample_len:test_end].reset_index()\n",
    "test_df = df[test_begin:test_end]\n",
    "x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "for i in range(sample_len,len(x_test_set)):\n",
    "    x_test.append(x_test_set[i-sample_len:i])\n",
    "    y_test.append(test_set['label'][i-1])\n",
    "test_df['y_true'] = y_test\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "y_test = to_categorical(y_test,2)\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "test_df['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pre-adjustment accuracy: 0.5425\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 109617.00\nSharpe: 0.46\nMax drawdown: 17.60%\nAnnual rate: 5.96%\nAdjusted accuracy: 0.4950\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 91781.00\nSharpe: -0.47\nMax drawdown: 16.58%\nAnnual rate: -5.26%\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(-0.4711286965132542, 16.581685692504227, -5.259809271875554)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "fit_window = 100\n",
    "weighted = False\n",
    "\n",
    "prob_save = []\n",
    "# test_df = df[test_begin:test_end]\n",
    "# test_df['y_true'] = y_true\n",
    "# test_df['y_pred'] = y_pred\n",
    "test_df['suc_num'] = get_suc_num(test_df)\n",
    "\n",
    "win_rate = [1] * fit_window\n",
    "for i in range(fit_window, len(test_df)):\n",
    "    this_true = test_df['y_true'].iloc[i-fit_window:i]\n",
    "    this_pred = test_df['y_pred'].iloc[i-fit_window:i]\n",
    "    this_prob = trans_prob(this_true,this_pred,weighted)\n",
    "    last_suc = test_df['suc_num'].iloc[i-1]\n",
    "    if last_suc not in this_prob.keys():\n",
    "        if last_suc > 0:\n",
    "            this_win = 0\n",
    "        else:\n",
    "            this_win = 1\n",
    "    else:\n",
    "        this_win = this_prob[last_suc] \n",
    "    prob_save.append(this_prob)\n",
    "    win_rate.append(this_win)\n",
    "test_df['win_rate'] = win_rate\n",
    "test_df['adjusted_pred'] = test_df['y_pred']\n",
    "test_df['adjusted_pred'].loc[test_df['win_rate']<0.5] = 1 - test_df['adjusted_pred'].loc[test_df['win_rate']<0.5]\n",
    "\n",
    "pre_acc = accuracy_score(test_df['y_true'].iloc[fit_window:],test_df['y_pred'].iloc[fit_window:])\n",
    "after_acc = accuracy_score(test_df['y_true'].iloc[fit_window:],test_df['adjusted_pred'].iloc[fit_window:])\n",
    "# test_df = orig_df[test_begin+fit_window:test_end]\n",
    "# test_df['label'] = y_pred\n",
    "# print(accuracy_score(y_true,y_pred))\n",
    "\n",
    "# adjusted_df = orig_df[test_begin+fit_window:test_end]\n",
    "# adjusted_df['label'] = test_df['adjusted_pred']\n",
    "# print(accuracy_score(y_true,adjusted_df['label']))\n",
    "print(f'Pre-adjustment accuracy: {pre_acc:.4f}')\n",
    "test_df['label'] = test_df['y_pred'].shift(-1).fillna(0)\n",
    "my_backtest(test_df.iloc[fit_window:])\n",
    "print(f'Adjusted accuracy: {after_acc:.4f}')\n",
    "test_df['label'] = test_df['adjusted_pred'].shift(-1).fillna(0)\n",
    "my_backtest(test_df.iloc[fit_window:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "( 0    0.000000\n  1    0.095009\n  2    0.103388\n  3    0.169895\n  4    0.179563\n  5    0.384963\n  6    0.484780\n  7    0.083611\n -1    0.064702\n -2    0.107350\n -3    0.244922\n -4    0.319008\n -5    0.114201\n -6    0.501110\n -7    0.404424\n  8    0.495712\n  9    0.000000\n -8    0.105409\n -9    0.000000\n dtype: float64,  0    0.000000\n  1    0.487604\n  2    0.527010\n  3    0.560913\n  4    0.676250\n  5    0.577083\n  6    0.484405\n  7    0.991573\n -1    0.558075\n -2    0.617284\n -3    0.611524\n -4    0.625183\n -5    0.013158\n -6    0.497778\n -7    0.203540\n  8    0.497175\n  9    0.000000\n -8    0.011111\n -9    1.000000\n dtype: float64)"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "a = pd.DataFrame(prob_save)\n",
    "a.std(),a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['log_profit'] = 2*(test_df['y_pred']-0.5)*test_df['log_r']\n",
    "win_profit = test_df['log_profit'].loc[test_df['y_true']==test_df['y_pred']].mean()\n",
    "lose_profit = test_df['log_profit'].loc[test_df['y_true']!=test_df['y_pred']].mean()\n",
    "pre_wtl = abs(win_profit / lose_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_win_profit = test_df['log_profit'].loc[test_df['y_true']==test_df['adjusted_pred']].mean()\n",
    "adj_lose_profit = test_df['log_profit'].loc[test_df['y_true']!=test_df['adjusted_pred']].mean()\n",
    "adj_wtl = abs(adj_win_profit / adj_lose_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.009514747478374845,\n -0.009302221590743646,\n 1.0228467883244876,\n 0.0054130332345014775,\n -0.003959310210107463,\n 1.3671657302029279)"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "win_profit,lose_profit,pre_wtl,adj_win_profit,adj_lose_profit,adj_wtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 浮动阈值法\n",
    "## 先计算出全部输出概率值，然后挑取大者观察\n",
    "test_len = 500\n",
    "test_begin = train_end\n",
    "test_end = test_begin + test_len\n",
    "\n",
    "x_test, y_test = [], []\n",
    "test_set = df[['log_r','label']][test_begin-sample_len:test_end].reset_index()\n",
    "test_set = df[['log_r','label']][test_begin-sample_len:test_end].reset_index()\n",
    "test_df = df[test_begin:test_end]\n",
    "x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "for i in range(sample_len,len(x_test_set)):\n",
    "    x_test.append(x_test_set[i-sample_len:i])\n",
    "    y_test.append(test_set['label'][i-1])\n",
    "test_df['y_true'] = y_test\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "y_test = to_categorical(y_test,2)\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "test_df['y_pred'] = y_pred\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# test_df = orig_df[test_begin:test_end]\n",
    "# test_df['y_true'] = y_true\n",
    "# test_df['y_pred'] = y_pred\n",
    "test_df['win'] = -1\n",
    "test_df['win'].loc[test_df['y_true']==test_df['y_pred']] = 1\n",
    "test_df['max_conf'] = y_pred_prob.max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pre-adjustment accuracy: 0.5425\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 109617.00\nSharpe: 0.46\nMax drawdown: 17.60%\nAnnual rate: 5.96%\n(0.46024613121331, 17.60142917587321, 5.955397383610872)\nAdjusted accuracy: 0.5000\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 92843.00\nSharpe: -0.43\nMax drawdown: 14.57%\nAnnual rate: -4.57%\n(-0.42507666032998365, 14.56583715873399, -4.5706481633274665)\n"
    }
   ],
   "source": [
    "fit_window = 100\n",
    "trust_thres = [0.5] * fit_window\n",
    "for i in range(fit_window, len(test_df)):\n",
    "    this_df = test_df.iloc[i-fit_window:i]\n",
    "    this_win = this_df['max_conf'].loc[this_df['win']==1].mean()\n",
    "    this_lose = this_df['max_conf'].loc[this_df['win']==-1].mean()\n",
    "    if this_win > this_lose:\n",
    "        threshold = (this_win + this_lose) * 0.5\n",
    "        trust_thres.append(threshold)\n",
    "    else:\n",
    "        trust_thres.append(1)\n",
    "\n",
    "test_df['trust_thres'] = threshold\n",
    "test_df['adjusted_pred'] = y_pred\n",
    "test_df['adjusted_pred'].loc[test_df['max_conf'] < test_df['trust_thres']] = 1 - test_df['adjusted_pred'].loc[test_df['max_conf'] < test_df['trust_thres']]\n",
    "\n",
    "# print(accuracy_score(test_df['y_true'].iloc[fit_window:],test_df['y_pred'].iloc[fit_window:]))\n",
    "# print(accuracy_score(test_df['y_true'].iloc[fit_window:],test_df['adjusted_pred'].iloc[fit_window:]))\n",
    "\n",
    "pre_acc = accuracy_score(test_df['y_true'].iloc[fit_window:],test_df['y_pred'].iloc[fit_window:])\n",
    "after_acc = accuracy_score(test_df['y_true'].iloc[fit_window:],test_df['adjusted_pred'].iloc[fit_window:])\n",
    "\n",
    "print(f'Pre-adjustment accuracy: {pre_acc:.4f}')\n",
    "test_df['label'] = test_df['y_pred'].shift(-1).fillna(0)\n",
    "print(my_backtest(test_df.iloc[fit_window:]))\n",
    "print(f'Adjusted accuracy: {after_acc:.4f}')\n",
    "test_df['label'] = test_df['adjusted_pred'].shift(-1).fillna(0)\n",
    "print(my_backtest(test_df.iloc[fit_window:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.008513103102104914,\n -0.008501713721241252,\n 1.0013396570664579,\n 0.0002507606129606986,\n -0.0002651531370280987,\n 0.9457199555369586)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "test_df['log_profit'] = 2*(test_df['y_pred']-0.5)*test_df['log_r']\n",
    "win_profit = test_df['log_profit'].loc[test_df['y_true']==test_df['y_pred']].mean()\n",
    "lose_profit = test_df['log_profit'].loc[test_df['y_true']!=test_df['y_pred']].mean()\n",
    "pre_wtl = abs(win_profit / lose_profit)\n",
    "adj_win_profit = test_df['log_profit'].loc[test_df['y_true']==test_df['adjusted_pred']].mean()\n",
    "adj_lose_profit = test_df['log_profit'].loc[test_df['y_true']!=test_df['adjusted_pred']].mean()\n",
    "adj_wtl = abs(adj_win_profit / adj_lose_profit)\n",
    "win_profit,lose_profit,pre_wtl,adj_win_profit,adj_lose_profit,adj_wtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit39c99c26652f4524b29e55ad15e6988f",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}