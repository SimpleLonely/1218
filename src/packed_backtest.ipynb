{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit39c99c26652f4524b29e55ad15e6988f",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 269972.00\nSharpe: 5.45\nMax drawdown: 0.92%\nAnnual rate: 64.96%\n"
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "from bt_classes import *\n",
    "orig_df = pd.read_csv('../res/test_data.csv',index_col=0,parse_dates=True)\n",
    "my_backtest(orig_df[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "df = pd.read_csv('../res/input0130.csv')\n",
    "orig_df = pd.read_csv('../xau_1d_20y.csv')\n",
    "orig_df['date'] = pd.to_datetime(orig_df['date'])\n",
    "orig_df = orig_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "#importing required libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "# follow the literature\n",
    "# we don't use min-max scaling here, use partial mean-std scaling instead\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import chain\n",
    "\n",
    "# and we define our model here\n",
    "def lstm_model(para_a=42, para_b=17):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=para_a, dropout=0.1, return_sequences=True, input_shape=(240,1),activation='tanh'))# (25,15)-57, (42,17)-58\n",
    "    model.add(LSTM(units=para_b, dropout=0.08, activation='tanh'))\n",
    "    # model.add(Dropout(0.08))# 加了之后同原先效果差不多，（应该一定程度上）可以防止过拟合\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1, train for only one time (many many days, maybe 2000?) and test several times(let's try 250 days per test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6917 - accuracy: 0.5255\nEpoch 2/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6901 - accuracy: 0.5395\nEpoch 3/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6900 - accuracy: 0.5345\nEpoch 4/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6902 - accuracy: 0.5340\nEpoch 5/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6896 - accuracy: 0.5320\nEpoch 6/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6909 - accuracy: 0.5280\nEpoch 7/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6901 - accuracy: 0.5310\nEpoch 8/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6899 - accuracy: 0.5330\nEpoch 9/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6904 - accuracy: 0.5275\nEpoch 10/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6901 - accuracy: 0.5365\nEpoch 11/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6898 - accuracy: 0.5325\nEpoch 12/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6901 - accuracy: 0.5310\nEpoch 13/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6901 - accuracy: 0.5360\nEpoch 14/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6900 - accuracy: 0.5365\nEpoch 15/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6896 - accuracy: 0.5345\nEpoch 16/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6912 - accuracy: 0.5340\nEpoch 17/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6906 - accuracy: 0.5295\nEpoch 18/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6902 - accuracy: 0.5375\nEpoch 19/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6903 - accuracy: 0.5345\nEpoch 20/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6897 - accuracy: 0.5360\nEpoch 21/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6895 - accuracy: 0.5405\nEpoch 22/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6893 - accuracy: 0.5395\nEpoch 23/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6894 - accuracy: 0.5315\nEpoch 24/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6896 - accuracy: 0.5355\nEpoch 25/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6890 - accuracy: 0.5340\nEpoch 26/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6893 - accuracy: 0.5355\nEpoch 27/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6909 - accuracy: 0.5335\nEpoch 28/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6899 - accuracy: 0.5405\nEpoch 29/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6892 - accuracy: 0.5335\nEpoch 30/30\n2000/2000 [==============================] - 8s 4ms/step - loss: 0.6897 - accuracy: 0.5315\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1c5315c7a08>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_begin = 240\n",
    "train_end = train_begin + 2000\n",
    "scaler = StandardScaler()\n",
    "train_set = df[['log_r','label']][train_begin-240:train_end].reset_index()\n",
    "x_train, y_train = [], []\n",
    "x_train_set = list(chain.from_iterable(scaler.fit_transform(train_set['log_r'].values.reshape(-1,1))))\n",
    "for i in range(240,len(x_train_set)):\n",
    "    x_train.append(x_train_set[i-240:i])\n",
    "    y_train.append(train_set['label'][i])\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "y_train = to_categorical(y_train,2)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) \n",
    "\n",
    "model = lstm_model()\n",
    "model.fit(x_train,y_train,epochs=30, batch_size=100, callbacks=[EarlyStopping(monitor='loss',patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 197341.00\nSharpe: 1.03\nMax drawdown: 10.73%\nAnnual rate: 8.94%\n"
    }
   ],
   "source": [
    "train_df = orig_df[train_begin:train_end]\n",
    "train_df['label'] = model.predict_classes(x_train)\n",
    "my_backtest(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_5 (LSTM)                (None, 240, 42)           7392      \n_________________________________________________________________\nlstm_6 (LSTM)                (None, 17)                4080      \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 36        \n=================================================================\nTotal params: 11,508\nTrainable params: 11,508\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('newest_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_backtest(mydata, log=False, drawpic=False, iplot=False):\n",
    "    plt.rcParams['figure.figsize'] = 12,8\n",
    "    cerebro = bt.Cerebro()\n",
    "    data = MyPandasData(dataname=mydata)\n",
    "    cerebro.adddata(data)\n",
    "    if log:\n",
    "        cerebro.addstrategy(GoldStrategy)\n",
    "    else:\n",
    "        cerebro.addstrategy(GoldStrategy_nolog)\n",
    "    cerebro.addsizer(OptInvest)\n",
    "    init_value= 100000.0\n",
    "    cerebro.broker.setcash(init_value)\n",
    "    cerebro.broker.setcommission(commission=50,margin=1000,mult=100)\n",
    "    cerebro.addanalyzer(bt.analyzers.SharpeRatio_A, _name='sharpe')\n",
    "    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade')\n",
    "    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='dd')\n",
    "    cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')\n",
    "    cerebro.addanalyzer(bt.analyzers.AnnualReturn, _name='ar')\n",
    "    print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    thestrats = cerebro.run()\n",
    "    final_value = cerebro.broker.getvalue()\n",
    "    print('Final Portfolio Value: %.2f' % final_value)\n",
    "    if drawpic:\n",
    "        cerebro.plot(iplot=iplot,volume=False)\n",
    "    sharpe = thestrats[0].analyzers.sharpe.get_analysis()['sharperatio']\n",
    "    dd = thestrats[0].analyzers.dd.get_analysis()['max']['drawdown']\n",
    "    ar = thestrats[0].analyzers.returns.get_analysis()['rnorm100']\n",
    "    print('Sharpe: {:.2f}'.format(sharpe))\n",
    "    print('Max drawdown: {:.2f}%'.format(dd))\n",
    "    print('Annual rate: {:.2f}%'.format(ar))\n",
    "    return sharpe,dd,ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sa import *\n",
    "from utils import *\n",
    "\n",
    "# Please select the last activation layer.\n",
    "layer_names = ['lstm_6']\n",
    "\n",
    "default_upper_bound = 2000\n",
    "default_n_bucket = 1000\n",
    "default_n_classes = 2\n",
    "class Args(): #创建一个类\n",
    "    def __init__(self): #定义初始化信息。\n",
    "        self.is_classification = True\n",
    "        self.save_path = './tmp/'\n",
    "        self.d = 'lstm_r'\n",
    "        self.num_classes = 2\n",
    "        self.lsa = True\n",
    "        self.dsa = True\n",
    "        self.target = 'none'\n",
    "        self.batch_size = 128\n",
    "        self.var_threshold = 1e-5\n",
    "        self.upper_bound = 2000\n",
    "        self.n_bucket = 1000\n",
    "        self.is_classification = True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 250/250 [00:00<00:00, 2071.09it/s]\nkde: 100%|██████████| 2/2 [00:00<00:00, 1017.91it/s]\n  0%|          | 0/250 [00:00<?, ?it/s]\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from2240 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from2240] \u001b[0mFetching DSA\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from2240 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from2240] \u001b[0mFetching LSA\n100%|██████████| 250/250 [00:00<00:00, 3094.71it/s]\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.70, Accuracy: 0.51\nDsa Mean: 0.55, Var: 0.56, Max: 3.11, Min: 0.05\nLsa Mean: 33.56, Var: 191.21, Max: 1248.25, Min: -69.53\n 85%|████████▍ | 212/250 [00:00<00:00, 2104.59it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 117198.00\nSharpe: 6.48\nMax drawdown: 18.06%\nAnnual rate: 17.35%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from2490 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from2490] \u001b[0mFetching DSA\n100%|██████████| 250/250 [00:00<00:00, 2089.43it/s]\nkde: 100%|██████████| 2/2 [00:00<00:00, 1002.82it/s]\n100%|██████████| 250/250 [00:00<00:00, 3133.34it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from2490 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from2490] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.69, Accuracy: 0.56\nDsa Mean: 0.32, Var: 0.34, Max: 2.54, Min: 0.02\nLsa Mean: -65.26, Var: 6.43, Max: -10.33, Min: -71.50\n 85%|████████▌ | 213/250 [00:00<00:00, 2114.52it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 125463.00\nSharpe: 1.85\nMax drawdown: 11.68%\nAnnual rate: 25.69%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from2740 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from2740] \u001b[0mFetching DSA\n100%|██████████| 250/250 [00:00<00:00, 2106.39it/s]\nkde: 100%|██████████| 2/2 [00:00<00:00, 1014.46it/s]\n100%|██████████| 250/250 [00:00<00:00, 3094.69it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from2740 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from2740] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.69, Accuracy: 0.59\nDsa Mean: 0.21, Var: 0.21, Max: 1.52, Min: 0.03\nLsa Mean: -68.22, Var: 2.93, Max: -56.15, Min: -71.21\n100%|██████████| 250/250 [00:00<00:00, 2071.65it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 151510.00\nSharpe: 1.91\nMax drawdown: 8.08%\nAnnual rate: 52.01%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from2990 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from2990] \u001b[0mFetching DSA\n\nkde: 100%|██████████| 2/2 [00:00<00:00, 1007.64it/s]\n100%|██████████| 250/250 [00:00<00:00, 3171.63it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from2990 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from2990] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 6ms/step\nLoss: 0.69, Accuracy: 0.52\nDsa Mean: 0.39, Var: 0.57, Max: 3.78, Min: 0.03\nLsa Mean: -53.71, Var: 24.49, Max: 75.94, Min: -71.25\n 84%|████████▎ | 209/250 [00:00<00:00, 2074.85it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 76309.00\nSharpe: -0.73\nMax drawdown: 45.79%\nAnnual rate: -23.86%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from3240 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from3240] \u001b[0mFetching DSA\n100%|██████████| 250/250 [00:00<00:00, 2071.65it/s]\nkde: 100%|██████████| 2/2 [00:00<00:00, 1998.24it/s]\n100%|██████████| 250/250 [00:00<00:00, 2984.17it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from3240 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from3240] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.70, Accuracy: 0.50\nDsa Mean: 0.16, Var: 0.15, Max: 1.11, Min: 0.04\nLsa Mean: -47.58, Var: 94.99, Max: 599.48, Min: -71.52\n100%|██████████| 250/250 [00:00<00:00, 2088.93it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 73995.00\nSharpe: -0.74\nMax drawdown: 37.79%\nAnnual rate: -26.18%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from3490 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from3490] \u001b[0mFetching DSA\n\nkde: 100%|██████████| 2/2 [00:00<00:00, 2070.75it/s]\n100%|██████████| 250/250 [00:00<00:00, 3056.97it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from3490 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from3490] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.70, Accuracy: 0.51\nDsa Mean: 0.20, Var: 0.21, Max: 1.79, Min: 0.04\nLsa Mean: -56.83, Var: 44.36, Max: 527.08, Min: -70.80\n 86%|████████▌ | 214/250 [00:00<00:00, 2124.54it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 82333.00\nSharpe: -0.63\nMax drawdown: 25.76%\nAnnual rate: -17.79%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from3740 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from3740] \u001b[0mFetching DSA\n100%|██████████| 250/250 [00:00<00:00, 2089.50it/s]\nkde: 100%|██████████| 2/2 [00:00<00:00, 1987.35it/s]\n100%|██████████| 250/250 [00:00<00:00, 2880.36it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from3740 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from3740] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.71, Accuracy: 0.46\nDsa Mean: 0.15, Var: 0.15, Max: 1.02, Min: 0.03\nLsa Mean: -66.20, Var: 6.49, Max: -28.39, Min: -71.68\n100%|██████████| 250/250 [00:00<00:00, 2089.48it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 94926.00\nSharpe: -0.32\nMax drawdown: 19.10%\nAnnual rate: -5.11%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from3990 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from3990] \u001b[0mFetching DSA\n\nkde: 100%|██████████| 2/2 [00:00<00:00, 1002.58it/s]\n100%|██████████| 250/250 [00:00<00:00, 2785.22it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from3990 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from3990] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.70, Accuracy: 0.48\nDsa Mean: 0.20, Var: 0.27, Max: 2.46, Min: 0.02\nLsa Mean: -66.03, Var: 5.76, Max: -30.72, Min: -71.66\n100%|██████████| 250/250 [00:00<00:00, 2089.05it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 103104.00\nSharpe: 0.12\nMax drawdown: 17.00%\nAnnual rate: 3.13%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from4240 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from4240] \u001b[0mFetching DSA\n\nkde: 100%|██████████| 2/2 [00:00<00:00, 2062.10it/s]\n100%|██████████| 250/250 [00:00<00:00, 2848.50it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from4240 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from4240] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 6ms/step\nLoss: 0.70, Accuracy: 0.52\nDsa Mean: 0.18, Var: 0.19, Max: 1.70, Min: 0.03\nLsa Mean: -65.69, Var: 6.39, Max: -19.15, Min: -71.14\n100%|██████████| 250/250 [00:00<00:00, 2055.09it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 103233.00\nSharpe: 0.11\nMax drawdown: 20.68%\nAnnual rate: 3.26%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from4490 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from4490] \u001b[0mFetching DSA\n\nkde: 100%|██████████| 2/2 [00:00<00:00, 1002.58it/s]\n100%|██████████| 250/250 [00:00<00:00, 2949.03it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from4490 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from4490] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 7ms/step\nLoss: 0.69, Accuracy: 0.55\nDsa Mean: 0.12, Var: 0.07, Max: 0.35, Min: 0.02\nLsa Mean: -70.04, Var: 1.06, Max: -66.06, Min: -71.73\n100%|██████████| 250/250 [00:00<00:00, 2038.09it/s]Starting Portfolio Value: 100000.00\nFinal Portfolio Value: 107766.00\nSharpe: 1.49\nMax drawdown: 9.71%\nAnnual rate: 7.83%\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved dsa from4740 ATs, skip serving\u001b[0m\n\u001b[94m[dsa from4740] \u001b[0mFetching DSA\n\nkde: 100%|██████████| 2/2 [00:00<00:00, 2067.18it/s]\n100%|██████████| 250/250 [00:00<00:00, 2949.07it/s]\n\u001b[92mFound saved train ATs, skip serving\u001b[0m\n\u001b[92mFound saved lsa from4740 ATs, skip serving\u001b[0m\n\u001b[92mThe number of removed columns: 0\u001b[0m\n\u001b[94m[lsa from4740] \u001b[0mFetching LSA\n250/250 [==============================] - 2s 6ms/step\nLoss: 0.70, Accuracy: 0.48\nDsa Mean: 0.09, Var: 0.05, Max: 0.25, Min: 0.02\nLsa Mean: -70.53, Var: 0.99, Max: -65.65, Min: -71.83\nStarting Portfolio Value: 100000.00\nFinal Portfolio Value: 100260.00\nSharpe: -0.16\nMax drawdown: 17.46%\nAnnual rate: 0.26%\n"
    }
   ],
   "source": [
    "starter = range(2240,len(df)-250,250)\n",
    "all_results = []\n",
    "\n",
    "for test_begin in starter:\n",
    "    test_end = test_begin + 250\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    test_set = df[['log_r','label']][test_begin-240:test_end].reset_index()\n",
    "    x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_test_set)):\n",
    "        x_test.append(x_test_set[i-240:i])\n",
    "        y_test.append(test_set['label'][i])\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "    y_test = to_categorical(y_test,2)\n",
    "\n",
    "    test_dsa = fetch_dsa(model, x_train, x_test, 'dsa from{}'.format(test_begin), layer_names, args)\n",
    "    dsa_cov = get_sc(np.amin(test_dsa), 5, 20, test_dsa)\n",
    "    test_lsa = fetch_lsa(model, x_train, x_test, 'lsa from{}'.format(test_begin), layer_names, args)\n",
    "    lsa_cov = get_sc(np.amin(test_lsa), 1500, 20, test_lsa)\n",
    "    eva = model.evaluate(x_test, y_test)\n",
    "    print('Loss: {:.2f}, Accuracy: {:.2f}'.format(eva[0], eva[1]))\n",
    "    print('Dsa Mean: {:.2f}, Var: {:.2f}, Max: {:.2f}, Min: {:.2f}'.format(np.mean(test_dsa),np.std(test_dsa),np.max(test_dsa),np.min(test_dsa)))\n",
    "    print('Lsa Mean: {:.2f}, Var: {:.2f}, Max: {:.2f}, Min: {:.2f}'.format(np.mean(test_lsa),np.std(test_lsa),np.max(test_lsa),np.min(test_lsa)))\n",
    "    test_df = orig_df[test_begin:test_end]\n",
    "    test_df['label'] = model.predict_classes(x_test)\n",
    "    sharpe,dd,ar = my_backtest(test_df)\n",
    "    this_result = [test_begin,eva[0], eva[1],dsa_cov,np.mean(test_dsa),np.std(test_dsa),np.max(test_dsa),np.min(test_dsa),lsa_cov,np.mean(test_lsa),np.std(test_lsa),np.max(test_lsa),np.min(test_lsa),sharpe,dd,ar]\n",
    "    all_results.append(this_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['test_begin','loss', 'accu','dsa_cov','dsa_mean','dsa_std','dsa_max','dsa_min','lsa_cov','lsa_mean','lsa_std','lsa_max','lsa_min','sharpe','drawdown','annual_return']\n",
    "result_df = pd.DataFrame(all_results,columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    test_begin      loss   accu  dsa_cov  dsa_mean   dsa_std   dsa_max  \\\n0         2240  0.698456  0.508     55.0  0.546526  0.561222  3.114424   \n1         2490  0.686643  0.560     40.0  0.317835  0.340786  2.542331   \n2         2740  0.686288  0.592     30.0  0.212340  0.210874  1.519107   \n3         2990  0.692316  0.516     60.0  0.386541  0.567986  3.778342   \n4         3240  0.695842  0.496     20.0  0.159306  0.151618  1.108419   \n5         3490  0.695469  0.512     30.0  0.203545  0.206412  1.790440   \n6         3740  0.705449  0.460     20.0  0.150610  0.151649  1.019424   \n7         3990  0.703024  0.476     40.0  0.198916  0.273698  2.456577   \n8         4240  0.698567  0.520     30.0  0.177066  0.187026  1.695585   \n9         4490  0.686407  0.548     10.0  0.116627  0.065245  0.348076   \n10        4740  0.700364  0.480      5.0  0.094039  0.045286  0.252162   \n\n     dsa_min  lsa_cov   lsa_mean     lsa_std      lsa_max    lsa_min  \\\n0   0.054373     70.0  33.559475  191.209889  1248.247018 -69.529467   \n1   0.023308      5.0 -65.261773    6.426795   -10.327409 -71.499709   \n2   0.026104      5.0 -68.219279    2.929719   -56.150657 -71.213576   \n3   0.034636     10.0 -53.712047   24.489958    75.937300 -71.245531   \n4   0.041060     40.0 -47.579933   94.988364   599.480046 -71.516060   \n5   0.041658     20.0 -56.834929   44.355334   527.081173 -70.800888   \n6   0.027984      5.0 -66.203580    6.490542   -28.385470 -71.683290   \n7   0.021010      5.0 -66.028288    5.759454   -30.715562 -71.662290   \n8   0.026664      5.0 -65.692088    6.393560   -19.154091 -71.137105   \n9   0.019122      5.0 -70.037435    1.063769   -66.059516 -71.731027   \n10  0.018321      5.0 -70.532899    0.993003   -65.649569 -71.826982   \n\n      sharpe   drawdown  annual_return  \n0   6.480774  18.061717      17.346884  \n1   1.854561  11.677169      25.690888  \n2   1.907296   8.084511      52.014435  \n3  -0.725497  45.788927     -23.855881  \n4  -0.742162  37.790400     -26.183068  \n5  -0.631019  25.755600     -17.794944  \n6  -0.321868  19.098936      -5.113536  \n7   0.115954  16.995238       3.129217  \n8   0.113021  20.682968       3.259281  \n9   1.487392   9.705368       7.830500  \n10 -0.164011  17.455218       0.262083  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test_begin</th>\n      <th>loss</th>\n      <th>accu</th>\n      <th>dsa_cov</th>\n      <th>dsa_mean</th>\n      <th>dsa_std</th>\n      <th>dsa_max</th>\n      <th>dsa_min</th>\n      <th>lsa_cov</th>\n      <th>lsa_mean</th>\n      <th>lsa_std</th>\n      <th>lsa_max</th>\n      <th>lsa_min</th>\n      <th>sharpe</th>\n      <th>drawdown</th>\n      <th>annual_return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2240</td>\n      <td>0.698456</td>\n      <td>0.508</td>\n      <td>55.0</td>\n      <td>0.546526</td>\n      <td>0.561222</td>\n      <td>3.114424</td>\n      <td>0.054373</td>\n      <td>70.0</td>\n      <td>33.559475</td>\n      <td>191.209889</td>\n      <td>1248.247018</td>\n      <td>-69.529467</td>\n      <td>6.480774</td>\n      <td>18.061717</td>\n      <td>17.346884</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2490</td>\n      <td>0.686643</td>\n      <td>0.560</td>\n      <td>40.0</td>\n      <td>0.317835</td>\n      <td>0.340786</td>\n      <td>2.542331</td>\n      <td>0.023308</td>\n      <td>5.0</td>\n      <td>-65.261773</td>\n      <td>6.426795</td>\n      <td>-10.327409</td>\n      <td>-71.499709</td>\n      <td>1.854561</td>\n      <td>11.677169</td>\n      <td>25.690888</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2740</td>\n      <td>0.686288</td>\n      <td>0.592</td>\n      <td>30.0</td>\n      <td>0.212340</td>\n      <td>0.210874</td>\n      <td>1.519107</td>\n      <td>0.026104</td>\n      <td>5.0</td>\n      <td>-68.219279</td>\n      <td>2.929719</td>\n      <td>-56.150657</td>\n      <td>-71.213576</td>\n      <td>1.907296</td>\n      <td>8.084511</td>\n      <td>52.014435</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2990</td>\n      <td>0.692316</td>\n      <td>0.516</td>\n      <td>60.0</td>\n      <td>0.386541</td>\n      <td>0.567986</td>\n      <td>3.778342</td>\n      <td>0.034636</td>\n      <td>10.0</td>\n      <td>-53.712047</td>\n      <td>24.489958</td>\n      <td>75.937300</td>\n      <td>-71.245531</td>\n      <td>-0.725497</td>\n      <td>45.788927</td>\n      <td>-23.855881</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3240</td>\n      <td>0.695842</td>\n      <td>0.496</td>\n      <td>20.0</td>\n      <td>0.159306</td>\n      <td>0.151618</td>\n      <td>1.108419</td>\n      <td>0.041060</td>\n      <td>40.0</td>\n      <td>-47.579933</td>\n      <td>94.988364</td>\n      <td>599.480046</td>\n      <td>-71.516060</td>\n      <td>-0.742162</td>\n      <td>37.790400</td>\n      <td>-26.183068</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3490</td>\n      <td>0.695469</td>\n      <td>0.512</td>\n      <td>30.0</td>\n      <td>0.203545</td>\n      <td>0.206412</td>\n      <td>1.790440</td>\n      <td>0.041658</td>\n      <td>20.0</td>\n      <td>-56.834929</td>\n      <td>44.355334</td>\n      <td>527.081173</td>\n      <td>-70.800888</td>\n      <td>-0.631019</td>\n      <td>25.755600</td>\n      <td>-17.794944</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3740</td>\n      <td>0.705449</td>\n      <td>0.460</td>\n      <td>20.0</td>\n      <td>0.150610</td>\n      <td>0.151649</td>\n      <td>1.019424</td>\n      <td>0.027984</td>\n      <td>5.0</td>\n      <td>-66.203580</td>\n      <td>6.490542</td>\n      <td>-28.385470</td>\n      <td>-71.683290</td>\n      <td>-0.321868</td>\n      <td>19.098936</td>\n      <td>-5.113536</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3990</td>\n      <td>0.703024</td>\n      <td>0.476</td>\n      <td>40.0</td>\n      <td>0.198916</td>\n      <td>0.273698</td>\n      <td>2.456577</td>\n      <td>0.021010</td>\n      <td>5.0</td>\n      <td>-66.028288</td>\n      <td>5.759454</td>\n      <td>-30.715562</td>\n      <td>-71.662290</td>\n      <td>0.115954</td>\n      <td>16.995238</td>\n      <td>3.129217</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4240</td>\n      <td>0.698567</td>\n      <td>0.520</td>\n      <td>30.0</td>\n      <td>0.177066</td>\n      <td>0.187026</td>\n      <td>1.695585</td>\n      <td>0.026664</td>\n      <td>5.0</td>\n      <td>-65.692088</td>\n      <td>6.393560</td>\n      <td>-19.154091</td>\n      <td>-71.137105</td>\n      <td>0.113021</td>\n      <td>20.682968</td>\n      <td>3.259281</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4490</td>\n      <td>0.686407</td>\n      <td>0.548</td>\n      <td>10.0</td>\n      <td>0.116627</td>\n      <td>0.065245</td>\n      <td>0.348076</td>\n      <td>0.019122</td>\n      <td>5.0</td>\n      <td>-70.037435</td>\n      <td>1.063769</td>\n      <td>-66.059516</td>\n      <td>-71.731027</td>\n      <td>1.487392</td>\n      <td>9.705368</td>\n      <td>7.830500</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4740</td>\n      <td>0.700364</td>\n      <td>0.480</td>\n      <td>5.0</td>\n      <td>0.094039</td>\n      <td>0.045286</td>\n      <td>0.252162</td>\n      <td>0.018321</td>\n      <td>5.0</td>\n      <td>-70.532899</td>\n      <td>0.993003</td>\n      <td>-65.649569</td>\n      <td>-71.826982</td>\n      <td>-0.164011</td>\n      <td>17.455218</td>\n      <td>0.262083</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}