{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16,9\n",
    "\n",
    "# read data, set index as dates, then plot price\n",
    "df = pd.read_csv('../res/input0130.csv')\n",
    "df.index = df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index, df['close'], label='Price History')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot returns\n",
    "plt.plot(df['date'], df['log_r'], label='Price History')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test some important properties\n",
    "import statsmodels.tsa.stattools as ts\n",
    "\n",
    "# acf and pacf\n",
    "# acf figure:\n",
    "plt.stem(ts.acf(df.log_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pacf figure:\n",
    "plt.stem(ts.pacf(df.log_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adf test\n",
    "print('adf test result:')\n",
    "print(ts.adfuller(df.log_r))\n",
    "print('kpss test result:')\n",
    "print(ts.kpss(df.log_r))\n",
    "# results are pretty good, so primarily we say it's wide-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "#importing required libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "# follow the literature\n",
    "# we don't use min-max scaling here, use partial mean-std scaling instead\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import chain\n",
    "\n",
    "# and we define our model here\n",
    "# def lstm_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(units=25, dropout=0.1, return_sequences=True, input_shape=(240,1), unroll=True))\n",
    "#     model.add(Activation(\"tanh\"))\n",
    "#     model.add(LSTM(units=25))\n",
    "#     model.add(Activation(\"tanh\"))\n",
    "#     model.add(Dense(2, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#     return model\n",
    "def lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, dropout=0.1, input_shape=(240,1), unroll=True))\n",
    "    # model.add(Activation(\"tanh\"))\n",
    "    # model.add(LSTM(units=25))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From D:\\Anaconda3\\envs\\inno\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nEpoch 1/50\n510/510 [==============================] - 12s 23ms/step - loss: 0.6935\nEpoch 2/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6929\nEpoch 3/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6894\nEpoch 4/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6870\nEpoch 5/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6857\nEpoch 6/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6874\nEpoch 7/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6856\nEpoch 8/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6836\nEpoch 9/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6846\nEpoch 10/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6835\nEpoch 11/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6813\nEpoch 12/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6797\nEpoch 13/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6749\nEpoch 14/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6782\nEpoch 15/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6809\nEpoch 16/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6763\nEpoch 17/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6767\nEpoch 18/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6722\nEpoch 19/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6741\nEpoch 20/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6688\nEpoch 21/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6744\nEpoch 22/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6769\nEpoch 23/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6663\nEpoch 24/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6651\nEpoch 25/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6660\nEpoch 26/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6630\nEpoch 27/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6660\nEpoch 28/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6625\nEpoch 29/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6646\nEpoch 30/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6569\nEpoch 31/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6563\nEpoch 32/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6641\nEpoch 33/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6572\nEpoch 34/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6629\nEpoch 35/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6633\nEpoch 36/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6496\nEpoch 37/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6525\nEpoch 38/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6591\nEpoch 39/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6561\nEpoch 40/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6503\nEpoch 41/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6431\nEpoch 42/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6494\nEpoch 43/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6537\nEpoch 44/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6508\nEpoch 45/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6457\nEpoch 46/50\n510/510 [==============================] - 1s 3ms/step - loss: 0.6343\nEpoch 47/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6355\nEpoch 48/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6413\nEpoch 49/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6304\nEpoch 50/50\n510/510 [==============================] - 1s 2ms/step - loss: 0.6401\n250/250 [==============================] - 2s 10ms/step\n"
    }
   ],
   "source": [
    "# we use rolling window, 750 bars for train and 250 for test\n",
    "scaler = StandardScaler()\n",
    "starter = range(240,len(df)-1000,250)\n",
    "all_results = []\n",
    "all_evas = []\n",
    "y_pred_clf = []\n",
    "for train_begin in starter:\n",
    "    train_end = train_begin + 750\n",
    "    test_end = train_end + 250\n",
    "    train_set = df[{'log_r','label'}][train_begin:train_end].reset_index()\n",
    "    x_train, y_train = [], []\n",
    "    x_train_set = list(chain.from_iterable(scaler.fit_transform(train_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_train_set)):\n",
    "        x_train.append(x_train_set[i-240:i])\n",
    "        y_train.append(train_set['label'][i])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    y_train = to_categorical(y_train,2)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) \n",
    "    \n",
    "    model = lstm_model()\n",
    "    model.fit(x_train,y_train,epochs=50,callbacks=[EarlyStopping(monitor='loss',patience=10)])\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    test_set = df[{'log_r','label'}][train_end-240:test_end].reset_index()\n",
    "    x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_test_set)):\n",
    "        x_test.append(x_test_set[i-240:i])\n",
    "        y_test.append(test_set['label'][i])\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "    y_test = to_categorical(y_test,2)\n",
    "    \n",
    "    all_results.append(model.predict(x_test))\n",
    "    all_evas.append(model.evaluate(x_test, y_test))\n",
    "    y_pred_clf.append(model.predict_classes(x_test))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.6137254901960785\n0.5725490196078431\n0.5235294117647059\n0.5333333333333333\n0.5431372549019607\n0.49411764705882355\n0.4764705882352941\n0.503921568627451\n0.5196078431372549\n0.5294117647058824\n0.4823529411764706\n0.4803921568627451\n0.5254901960784314\n0.5274509803921569\n0.5098039215686274\n0.515686274509804\n"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "for train_begin in starter:\n",
    "    train_end = train_begin + 750\n",
    "    test_end = train_end + 250\n",
    "    train_set = df[{'log_r','label'}][train_begin:train_end].reset_index()\n",
    "    x_train, y_train = [], []\n",
    "    x_train_set = list(chain.from_iterable(scaler.fit_transform(train_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_train_set)):\n",
    "        x_train.append(x_train_set[i-240:i])\n",
    "        y_train.append(train_set['label'][i])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    # y_train = to_categorical(y_train,2)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) \n",
    "    x_test, y_test = [], []\n",
    "    test_set = df[{'log_r','label'}][train_end-240:test_end].reset_index()\n",
    "    x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_test_set)):\n",
    "        x_test.append(x_test_set[i-240:i])\n",
    "        y_test.append(test_set['label'][i])\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "    y_pred = model.predict_classes(x_train)\n",
    "    # all_results.append(model.predict(x_test))\n",
    "    print(metrics.accuracy_score(y_train, y_pred))\n",
    "    # y_pred_clf.append(model.predict_classes(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "250/250 [==============================] - 2s 7ms/step\n"
    },
    {
     "data": {
      "text/plain": "0.92156671667099"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[0.58558035, 0.41441968],\n        [0.632435  , 0.36756495],\n        [0.7901696 , 0.20983036],\n        [0.8393114 , 0.16068858],\n        [0.7285606 , 0.27143937],\n        [0.76550853, 0.23449145],\n        [0.60321194, 0.396788  ],\n        [0.39562562, 0.60437435],\n        [0.8017488 , 0.19825116],\n        [0.71366394, 0.28633603],\n        [0.89344186, 0.10655807],\n        [0.55107474, 0.44892526],\n        [0.88251483, 0.1174852 ],\n        [0.9040997 , 0.09590032],\n        [0.72038823, 0.27961177],\n        [0.8730326 , 0.12696736],\n        [0.9119338 , 0.08806619],\n        [0.8963829 , 0.10361706],\n        [0.9506015 , 0.04939847],\n        [0.8860689 , 0.11393112],\n        [0.8829777 , 0.11702226],\n        [0.57153374, 0.42846626],\n        [0.90522677, 0.09477323],\n        [0.6834715 , 0.3165285 ],\n        [0.58991265, 0.41008735],\n        [0.5537586 , 0.44624138],\n        [0.15473409, 0.84526587],\n        [0.06548354, 0.93451643],\n        [0.0983642 , 0.9016358 ],\n        [0.07603713, 0.9239629 ],\n        [0.12858549, 0.8714145 ],\n        [0.3026099 , 0.69739014],\n        [0.3858953 , 0.61410475],\n        [0.44357184, 0.55642813],\n        [0.3362594 , 0.6637406 ],\n        [0.5435531 , 0.4564469 ],\n        [0.51051146, 0.48948857],\n        [0.5623406 , 0.43765935],\n        [0.38755462, 0.61244535],\n        [0.42590562, 0.5740944 ],\n        [0.7333391 , 0.2666609 ],\n        [0.70927066, 0.29072934],\n        [0.5292518 , 0.47074822],\n        [0.8144883 , 0.1855117 ],\n        [0.6406146 , 0.3593854 ],\n        [0.53397816, 0.46602187],\n        [0.48883197, 0.51116806],\n        [0.42839968, 0.5716003 ],\n        [0.4010147 , 0.59898525],\n        [0.3938957 , 0.6061044 ],\n        [0.39241812, 0.6075819 ],\n        [0.40438303, 0.59561694],\n        [0.42418352, 0.5758165 ],\n        [0.45072624, 0.5492738 ],\n        [0.44698098, 0.553019  ],\n        [0.4446721 , 0.5553279 ],\n        [0.3717304 , 0.6282696 ],\n        [0.41804785, 0.58195215],\n        [0.4295234 , 0.5704766 ],\n        [0.37737745, 0.6226226 ],\n        [0.36170313, 0.6382969 ],\n        [0.3665871 , 0.6334129 ],\n        [0.3499715 , 0.6500285 ],\n        [0.3810929 , 0.61890715],\n        [0.47365224, 0.52634776],\n        [0.59544057, 0.4045595 ],\n        [0.6767311 , 0.32326886],\n        [0.7934796 , 0.20652032],\n        [0.76416785, 0.23583221],\n        [0.66260177, 0.33739823],\n        [0.50283426, 0.49716577],\n        [0.3015831 , 0.6984168 ],\n        [0.3081956 , 0.6918044 ],\n        [0.34490365, 0.65509635],\n        [0.33239365, 0.6676064 ],\n        [0.3541729 , 0.6458271 ],\n        [0.3671103 , 0.6328897 ],\n        [0.3848444 , 0.6151556 ],\n        [0.42181122, 0.5781888 ],\n        [0.44198045, 0.5580196 ],\n        [0.47492322, 0.52507675],\n        [0.5133075 , 0.48669246],\n        [0.5082665 , 0.4917335 ],\n        [0.50374013, 0.4962599 ],\n        [0.48749906, 0.51250094],\n        [0.47991067, 0.5200893 ],\n        [0.5549645 , 0.44503555],\n        [0.6676359 , 0.3323641 ],\n        [0.6610429 , 0.33895707],\n        [0.78479654, 0.21520345],\n        [0.681203  , 0.31879696],\n        [0.8044992 , 0.19550085],\n        [0.941165  , 0.05883504],\n        [0.61138594, 0.38861406],\n        [0.4406054 , 0.55939466],\n        [0.34252793, 0.6574721 ],\n        [0.43103012, 0.5689699 ],\n        [0.09014165, 0.90985835],\n        [0.08027362, 0.9197264 ],\n        [0.07948285, 0.9205171 ],\n        [0.4189789 , 0.5810211 ],\n        [0.53499025, 0.46500972],\n        [0.6517109 , 0.34828904],\n        [0.4484176 , 0.55158234],\n        [0.68452597, 0.315474  ],\n        [0.21561997, 0.78438   ],\n        [0.7387274 , 0.26127258],\n        [0.7726869 , 0.22731313],\n        [0.886989  , 0.11301096],\n        [0.889519  , 0.11048099],\n        [0.81198907, 0.18801092],\n        [0.91540796, 0.08459207],\n        [0.89982116, 0.10017884],\n        [0.69757307, 0.30242696],\n        [0.8488621 , 0.15113786],\n        [0.9662058 , 0.03379422],\n        [0.72956234, 0.27043766],\n        [0.9005883 , 0.09941168],\n        [0.9470994 , 0.05290058],\n        [0.81366247, 0.18633747],\n        [0.39895952, 0.6010405 ],\n        [0.17164502, 0.828355  ],\n        [0.18267173, 0.8173283 ],\n        [0.15103807, 0.8489619 ],\n        [0.07373128, 0.9262687 ],\n        [0.04883633, 0.95116365],\n        [0.04416329, 0.95583665],\n        [0.04509174, 0.9549082 ],\n        [0.02910889, 0.9708911 ],\n        [0.02169753, 0.9783025 ],\n        [0.02142825, 0.9785718 ],\n        [0.02754403, 0.9724559 ],\n        [0.03722454, 0.96277547],\n        [0.04530733, 0.9546927 ],\n        [0.06845969, 0.9315403 ],\n        [0.14276634, 0.85723364],\n        [0.28394866, 0.7160514 ],\n        [0.4563533 , 0.5436467 ],\n        [0.5149179 , 0.48508206],\n        [0.59642935, 0.4035707 ],\n        [0.64173377, 0.35826626],\n        [0.68164843, 0.31835163],\n        [0.7435848 , 0.2564152 ],\n        [0.74545   , 0.25454998],\n        [0.6176109 , 0.3823891 ],\n        [0.4834617 , 0.51653826],\n        [0.633566  , 0.366434  ],\n        [0.68736154, 0.31263846],\n        [0.76198846, 0.23801157],\n        [0.6302911 , 0.3697089 ],\n        [0.7069267 , 0.2930733 ],\n        [0.77660394, 0.22339606],\n        [0.55582887, 0.44417113],\n        [0.7657493 , 0.2342507 ],\n        [0.46953148, 0.5304686 ],\n        [0.678878  , 0.32112193],\n        [0.59735566, 0.4026443 ],\n        [0.622136  , 0.37786397],\n        [0.47616187, 0.5238381 ],\n        [0.6493228 , 0.35067722],\n        [0.55977446, 0.44022548],\n        [0.6968345 , 0.30316547],\n        [0.724426  , 0.275574  ],\n        [0.6819136 , 0.3180864 ],\n        [0.5216632 , 0.47833684],\n        [0.4939761 , 0.50602394],\n        [0.5638612 , 0.43613884],\n        [0.20102695, 0.798973  ],\n        [0.03937479, 0.96062523],\n        [0.0342848 , 0.9657152 ],\n        [0.02471272, 0.9752873 ],\n        [0.02319479, 0.9768052 ],\n        [0.0678833 , 0.93211675],\n        [0.07593493, 0.92406505],\n        [0.22889614, 0.7711039 ],\n        [0.60484505, 0.39515495],\n        [0.6446015 , 0.35539848],\n        [0.65170515, 0.34829488],\n        [0.6180857 , 0.38191432],\n        [0.7317169 , 0.26828313],\n        [0.58687305, 0.41312695],\n        [0.47667563, 0.5233244 ],\n        [0.2597152 , 0.7402848 ],\n        [0.7491712 , 0.25082883],\n        [0.8998416 , 0.10015834],\n        [0.9341857 , 0.06581433],\n        [0.87948906, 0.12051094],\n        [0.43777782, 0.5622222 ],\n        [0.9798978 , 0.02010226],\n        [0.9849582 , 0.01504178],\n        [0.9403328 , 0.05966727],\n        [0.9091713 , 0.09082866],\n        [0.5507578 , 0.44924217],\n        [0.92741305, 0.07258695],\n        [0.7600319 , 0.23996809],\n        [0.8698148 , 0.13018522],\n        [0.67173266, 0.3282674 ],\n        [0.3049659 , 0.6950341 ],\n        [0.14194989, 0.85805017],\n        [0.06019249, 0.9398075 ],\n        [0.03106552, 0.9689344 ],\n        [0.02052192, 0.9794781 ],\n        [0.01790397, 0.9820961 ],\n        [0.01726004, 0.9827399 ],\n        [0.01991157, 0.9800884 ],\n        [0.03179635, 0.9682036 ],\n        [0.07367244, 0.92632765],\n        [0.18619776, 0.8138022 ],\n        [0.33426827, 0.66573167],\n        [0.43541977, 0.56458026],\n        [0.52543765, 0.47456235],\n        [0.567524  , 0.432476  ],\n        [0.52565795, 0.47434205],\n        [0.4585861 , 0.54141396],\n        [0.43414128, 0.5658587 ],\n        [0.5494814 , 0.4505186 ],\n        [0.612986  , 0.38701394],\n        [0.62558264, 0.3744173 ],\n        [0.51131   , 0.48869005],\n        [0.5531703 , 0.4468297 ],\n        [0.696543  , 0.30345702],\n        [0.48291758, 0.5170824 ],\n        [0.39006212, 0.6099379 ],\n        [0.8425582 , 0.1574418 ],\n        [0.8782276 , 0.12177236],\n        [0.7200885 , 0.27991152],\n        [0.9383522 , 0.06164781],\n        [0.9265931 , 0.07340682],\n        [0.8018095 , 0.19819047],\n        [0.5590196 , 0.44098035],\n        [0.2616517 , 0.73834825],\n        [0.25543374, 0.74456626],\n        [0.09330964, 0.9066904 ],\n        [0.05146199, 0.94853795],\n        [0.02935561, 0.9706444 ],\n        [0.03714586, 0.96285415],\n        [0.07148957, 0.9285104 ],\n        [0.16223328, 0.83776665],\n        [0.28396043, 0.71603954],\n        [0.37558973, 0.6244103 ],\n        [0.43600345, 0.56399655],\n        [0.45706668, 0.54293334],\n        [0.46932885, 0.5306712 ],\n        [0.45481586, 0.5451841 ],\n        [0.43481207, 0.56518793],\n        [0.4064668 , 0.59353316],\n        [0.39375928, 0.6062407 ],\n        [0.41079476, 0.5892052 ],\n        [0.40494078, 0.5950592 ],\n        [0.42287642, 0.5771236 ]], dtype=float32)]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[0.92156671667099]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_r_1c.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, 50)                10400     \n_________________________________________________________________\nactivation_1 (Activation)    (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 102       \n=================================================================\nTotal params: 10,502\nTrainable params: 10,502\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# see what's in model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from keras.models import load_model, Model\n",
    "from sa import fetch_dsa, fetch_lsa, get_sc, \n",
    "from utils import *\n",
    "\n",
    "model = load_model('lstm_r.h5')\n",
    "default_upper_bound = 2000\n",
    "default_n_bucket = 1000\n",
    "default_n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(): #创建一个类\n",
    "    def __init__(self): #定义初始化信息。\n",
    "        self.is_classification = True\n",
    "        self.save_path = './tmp/'\n",
    "        self.d = 'lstm_r'\n",
    "        self.num_classes = 2\n",
    "        self.lsa = True\n",
    "        self.dsa = True\n",
    "        self.target = 'none'\n",
    "        self.batch_size = 128\n",
    "        self.var_threshold = 1e-5\n",
    "        self.upper_bound = 2000\n",
    "        self.n_bucket = 1000\n",
    "        self.num_classes = 10\n",
    "        self.is_classification = True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_lsa(model, x_train, x_target, target_name, layer_names, args):\n",
    "    \"\"\"Likelihood-based SA\n",
    "\n",
    "    Args:\n",
    "        model (keras model): Subject model.\n",
    "        x_train (list): Set of training inputs.\n",
    "        x_target (list): Set of target (test or[] adversarial) inputs.\n",
    "        target_name (str): Name of target set.\n",
    "        layer_names (list): List of selected layer names.\n",
    "        args: Keyboard args.\n",
    "\n",
    "    Returns:\n",
    "        lsa (list): List of lsa for each target input.\n",
    "    \"\"\"\n",
    "\n",
    "    prefix = info(\"[\" + target_name + \"] \")\n",
    "    train_ats, train_pred, target_ats, target_pred = _get_train_target_ats(\n",
    "        model, x_train, x_target, target_name, layer_names, args\n",
    "    )\n",
    "\n",
    "    class_matrix = {}\n",
    "    if args.is_classification:\n",
    "        for i, label in enumerate(train_pred):\n",
    "            label = label[0]\n",
    "            if label not in class_matrix:\n",
    "                class_matrix[label] = []\n",
    "            class_matrix[label].append(i)\n",
    "\n",
    "    kdes, removed_cols = _get_kdes(train_ats, train_pred, class_matrix, args)\n",
    "\n",
    "    lsa = []\n",
    "    print(prefix + \"Fetching LSA\")\n",
    "    if args.is_classification:\n",
    "        for i, at in enumerate(tqdm(target_ats)):\n",
    "            label = target_pred[i]\n",
    "            kde = kdes[label]\n",
    "            lsa.append(_get_lsa(kde, at, removed_cols))\n",
    "    else:\n",
    "        kde = kdes[0]\n",
    "        for at in tqdm(target_ats):\n",
    "            lsa.append(_get_lsa(kde, at, removed_cols))\n",
    "\n",
    "    return lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['lstm_34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.predict_classes(x_test)\n",
    "# matrix = {}\n",
    "# for i,label in enumerate(pred):\n",
    "#     if label[0] not in matrix:\n",
    "#         print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lsa = fetch_lsa(model, x_train, x_test, \"test\", layer_names, args)\n",
    "target_lsa = fetch_lsa(model, x_train, x_test, \"target\", layer_names, args)\n",
    "# target_lsa = fetch_lsa(model, x_train, x_target, args.target, layer_names, args)\n",
    "target_cov = get_sc(\n",
    "    np.amin(target_lsa), default_upper_bound, default_n_bucket, target_lsa\n",
    ")\n",
    "\n",
    "auc = compute_roc_auc(test_lsa, target_lsa)\n",
    "print(infog(\"ROC-AUC: \" + str(auc * 100)))\n",
    "\n",
    "print(infog(\"LSA coverage: \" + str(target_cov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dsa = fetch_dsa(model, x_train, x_test, \"test\", layer_names, args)\n",
    "\n",
    "target_dsa = fetch_dsa(model, x_train, x_test, args.target, layer_names, args)\n",
    "target_cov = get_sc(\n",
    "    np.amin(target_dsa), default_upper_bound, default_n_bucket, target_dsa\n",
    ")\n",
    "\n",
    "auc = compute_roc_auc(test_dsa, target_dsa)\n",
    "print(infog(\"ROC-AUC: \" + str(auc * 100)))\n",
    "\n",
    "print(infog(\"DSA coverage: \" + str(target_cov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.5-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}