{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16,9\n",
    "\n",
    "# read data, set index as dates, then plot price\n",
    "df = pd.read_csv('../res/input0130.csv')\n",
    "df.index = df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.index, df['close'], label='Price History')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot returns\n",
    "plt.plot(df['date'], df['log_r'], label='Price History')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test some important properties\n",
    "import statsmodels.tsa.stattools as ts\n",
    "\n",
    "# acf and pacf\n",
    "# acf figure:\n",
    "plt.stem(ts.acf(df.log_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pacf figure:\n",
    "plt.stem(ts.pacf(df.log_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adf test\n",
    "print('adf test result:')\n",
    "print(ts.adfuller(df.log_r))\n",
    "print('kpss test result:')\n",
    "print(ts.kpss(df.log_r))\n",
    "# results are pretty good, so primarily we say it's wide-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "# follow the literature\n",
    "# we don't use min-max scaling here, use partial mean-std scaling instead\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import chain\n",
    "\n",
    "# and we define our model here\n",
    "def lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=25, dropout=0.1, return_sequences=True, input_shape=(240,1)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(LSTM(units=25))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\inno\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "510/510 [==============================] - 6s 13ms/step - loss: 0.6950\n",
      "Epoch 2/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6928\n",
      "Epoch 3/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6904\n",
      "Epoch 4/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6917\n",
      "Epoch 5/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6900\n",
      "Epoch 6/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6891\n",
      "Epoch 7/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6903\n",
      "Epoch 8/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6887\n",
      "Epoch 9/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6904\n",
      "Epoch 10/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6882\n",
      "Epoch 11/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6872\n",
      "Epoch 12/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6883\n",
      "Epoch 13/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6901\n",
      "Epoch 14/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6900\n",
      "Epoch 15/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6895\n",
      "Epoch 16/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6878\n",
      "Epoch 17/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6847\n",
      "Epoch 18/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6870\n",
      "Epoch 19/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6849\n",
      "Epoch 20/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6830\n",
      "Epoch 21/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6812\n",
      "Epoch 22/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6837\n",
      "Epoch 23/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6819\n",
      "Epoch 24/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6865\n",
      "Epoch 25/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6788\n",
      "Epoch 26/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6810\n",
      "Epoch 27/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6841\n",
      "Epoch 28/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6750\n",
      "Epoch 29/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6785\n",
      "Epoch 30/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6754\n",
      "Epoch 31/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6768\n",
      "Epoch 32/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6747\n",
      "Epoch 33/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6708\n",
      "Epoch 34/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6650\n",
      "Epoch 35/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6711\n",
      "Epoch 36/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6682\n",
      "Epoch 37/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6726\n",
      "Epoch 38/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6831\n",
      "Epoch 39/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6717\n",
      "Epoch 40/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6717\n",
      "Epoch 41/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6684\n",
      "Epoch 42/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6687\n",
      "Epoch 43/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6632\n",
      "Epoch 44/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6647\n",
      "Epoch 45/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6639\n",
      "Epoch 46/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6597\n",
      "Epoch 47/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6704\n",
      "Epoch 48/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6617\n",
      "Epoch 49/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6562\n",
      "Epoch 50/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6475\n",
      "Epoch 51/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6493\n",
      "Epoch 52/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6505\n",
      "Epoch 53/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6577\n",
      "Epoch 54/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6479\n",
      "Epoch 55/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6574\n",
      "Epoch 56/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6434\n",
      "Epoch 57/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6448\n",
      "Epoch 58/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6396\n",
      "Epoch 59/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6362\n",
      "Epoch 60/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6313\n",
      "Epoch 61/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6362\n",
      "Epoch 62/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6323\n",
      "Epoch 63/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6355\n",
      "Epoch 64/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6378\n",
      "Epoch 65/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6466\n",
      "Epoch 66/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6388\n",
      "Epoch 67/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6277\n",
      "Epoch 68/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6223\n",
      "Epoch 69/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6285\n",
      "Epoch 70/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6166\n",
      "Epoch 71/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6311\n",
      "Epoch 72/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6281\n",
      "Epoch 73/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6112\n",
      "Epoch 74/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6130\n",
      "Epoch 75/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6167\n",
      "Epoch 76/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6166\n",
      "Epoch 77/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.5967\n",
      "Epoch 78/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6058\n",
      "Epoch 79/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6079\n",
      "Epoch 80/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6094\n",
      "Epoch 81/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.6165\n",
      "Epoch 82/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5995\n",
      "Epoch 83/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6003\n",
      "Epoch 84/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5964\n",
      "Epoch 85/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6022\n",
      "Epoch 86/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.6011\n",
      "Epoch 87/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5800\n",
      "Epoch 88/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5850\n",
      "Epoch 89/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5894\n",
      "Epoch 90/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5864\n",
      "Epoch 91/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5925\n",
      "Epoch 92/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5865\n",
      "Epoch 93/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5792\n",
      "Epoch 94/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5644\n",
      "Epoch 95/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.5934\n",
      "Epoch 96/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5964\n",
      "Epoch 97/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5754\n",
      "Epoch 98/100\n",
      "510/510 [==============================] - 6s 11ms/step - loss: 0.5722\n",
      "Epoch 99/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.5707\n",
      "Epoch 100/100\n",
      "510/510 [==============================] - 6s 12ms/step - loss: 0.5724\n",
      "250/250 [==============================] - 2s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# we use rolling window, 750 bars for train and 250 for test\n",
    "scaler = StandardScaler()\n",
    "starter = range(240,len(df)-1000,250)\n",
    "all_results = []\n",
    "all_evas = []\n",
    "y_pred_clf = []\n",
    "for train_begin in starter:\n",
    "    train_end = train_begin + 750\n",
    "    test_end = train_end + 250\n",
    "    train_set = df[{'log_r','label'}][train_begin:train_end].reset_index()\n",
    "    x_train, y_train = [], []\n",
    "    x_train_set = list(chain.from_iterable(scaler.fit_transform(train_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_train_set)):\n",
    "        x_train.append(x_train_set[i-240:i])\n",
    "        y_train.append(train_set['label'][i])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    y_train = to_categorical(y_train,2)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) \n",
    "    \n",
    "    model = lstm_model()\n",
    "    model.fit(x_train,y_train,epochs=100,callbacks=[EarlyStopping(monitor='loss',patience=10)])\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "    test_set = df[{'log_r','label'}][train_end-240:test_end].reset_index()\n",
    "    x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_test_set)):\n",
    "        x_test.append(x_test_set[i-240:i])\n",
    "        y_test.append(test_set['label'][i])\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "    y_test = to_categorical(y_test,2)\n",
    "    \n",
    "    all_results.append(model.predict(x_test))\n",
    "    all_evas.append(model.evaluate(x_test, y_test))\n",
    "    y_pred_clf.append(model.predict_classes(x_test))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7137254901960784\n",
      "0.6254901960784314\n",
      "0.5450980392156862\n",
      "0.4980392156862745\n",
      "0.49607843137254903\n",
      "0.5313725490196078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-22839667ecc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m# all_results.append(model.predict(x_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\inno\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\inno\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\inno\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\inno\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\inno\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "for train_begin in starter:\n",
    "    train_end = train_begin + 750\n",
    "    test_end = train_end + 250\n",
    "    train_set = df[{'log_r','label'}][train_begin:train_end].reset_index()\n",
    "    x_train, y_train = [], []\n",
    "    x_train_set = list(chain.from_iterable(scaler.fit_transform(train_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_train_set)):\n",
    "        x_train.append(x_train_set[i-240:i])\n",
    "        y_train.append(train_set['label'][i])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    # y_train = to_categorical(y_train,2)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) \n",
    "    x_test, y_test = [], []\n",
    "    test_set = df[{'log_r','label'}][train_end-240:test_end].reset_index()\n",
    "    x_test_set = list(chain.from_iterable(scaler.transform(test_set['log_r'].values.reshape(-1,1))))\n",
    "    for i in range(240,len(x_test_set)):\n",
    "        x_test.append(x_test_set[i-240:i])\n",
    "        y_test.append(test_set['label'][i])\n",
    "    x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1)) \n",
    "    y_pred = model.predict_classes(x_train)\n",
    "    # all_results.append(model.predict(x_test))\n",
    "    print(metrics.accuracy_score(y_train, y_pred))\n",
    "    # y_pred_clf.append(model.predict_classes(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 2s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92156671667099"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.58558035, 0.41441968],\n",
       "        [0.632435  , 0.36756495],\n",
       "        [0.7901696 , 0.20983036],\n",
       "        [0.8393114 , 0.16068858],\n",
       "        [0.7285606 , 0.27143937],\n",
       "        [0.76550853, 0.23449145],\n",
       "        [0.60321194, 0.396788  ],\n",
       "        [0.39562562, 0.60437435],\n",
       "        [0.8017488 , 0.19825116],\n",
       "        [0.71366394, 0.28633603],\n",
       "        [0.89344186, 0.10655807],\n",
       "        [0.55107474, 0.44892526],\n",
       "        [0.88251483, 0.1174852 ],\n",
       "        [0.9040997 , 0.09590032],\n",
       "        [0.72038823, 0.27961177],\n",
       "        [0.8730326 , 0.12696736],\n",
       "        [0.9119338 , 0.08806619],\n",
       "        [0.8963829 , 0.10361706],\n",
       "        [0.9506015 , 0.04939847],\n",
       "        [0.8860689 , 0.11393112],\n",
       "        [0.8829777 , 0.11702226],\n",
       "        [0.57153374, 0.42846626],\n",
       "        [0.90522677, 0.09477323],\n",
       "        [0.6834715 , 0.3165285 ],\n",
       "        [0.58991265, 0.41008735],\n",
       "        [0.5537586 , 0.44624138],\n",
       "        [0.15473409, 0.84526587],\n",
       "        [0.06548354, 0.93451643],\n",
       "        [0.0983642 , 0.9016358 ],\n",
       "        [0.07603713, 0.9239629 ],\n",
       "        [0.12858549, 0.8714145 ],\n",
       "        [0.3026099 , 0.69739014],\n",
       "        [0.3858953 , 0.61410475],\n",
       "        [0.44357184, 0.55642813],\n",
       "        [0.3362594 , 0.6637406 ],\n",
       "        [0.5435531 , 0.4564469 ],\n",
       "        [0.51051146, 0.48948857],\n",
       "        [0.5623406 , 0.43765935],\n",
       "        [0.38755462, 0.61244535],\n",
       "        [0.42590562, 0.5740944 ],\n",
       "        [0.7333391 , 0.2666609 ],\n",
       "        [0.70927066, 0.29072934],\n",
       "        [0.5292518 , 0.47074822],\n",
       "        [0.8144883 , 0.1855117 ],\n",
       "        [0.6406146 , 0.3593854 ],\n",
       "        [0.53397816, 0.46602187],\n",
       "        [0.48883197, 0.51116806],\n",
       "        [0.42839968, 0.5716003 ],\n",
       "        [0.4010147 , 0.59898525],\n",
       "        [0.3938957 , 0.6061044 ],\n",
       "        [0.39241812, 0.6075819 ],\n",
       "        [0.40438303, 0.59561694],\n",
       "        [0.42418352, 0.5758165 ],\n",
       "        [0.45072624, 0.5492738 ],\n",
       "        [0.44698098, 0.553019  ],\n",
       "        [0.4446721 , 0.5553279 ],\n",
       "        [0.3717304 , 0.6282696 ],\n",
       "        [0.41804785, 0.58195215],\n",
       "        [0.4295234 , 0.5704766 ],\n",
       "        [0.37737745, 0.6226226 ],\n",
       "        [0.36170313, 0.6382969 ],\n",
       "        [0.3665871 , 0.6334129 ],\n",
       "        [0.3499715 , 0.6500285 ],\n",
       "        [0.3810929 , 0.61890715],\n",
       "        [0.47365224, 0.52634776],\n",
       "        [0.59544057, 0.4045595 ],\n",
       "        [0.6767311 , 0.32326886],\n",
       "        [0.7934796 , 0.20652032],\n",
       "        [0.76416785, 0.23583221],\n",
       "        [0.66260177, 0.33739823],\n",
       "        [0.50283426, 0.49716577],\n",
       "        [0.3015831 , 0.6984168 ],\n",
       "        [0.3081956 , 0.6918044 ],\n",
       "        [0.34490365, 0.65509635],\n",
       "        [0.33239365, 0.6676064 ],\n",
       "        [0.3541729 , 0.6458271 ],\n",
       "        [0.3671103 , 0.6328897 ],\n",
       "        [0.3848444 , 0.6151556 ],\n",
       "        [0.42181122, 0.5781888 ],\n",
       "        [0.44198045, 0.5580196 ],\n",
       "        [0.47492322, 0.52507675],\n",
       "        [0.5133075 , 0.48669246],\n",
       "        [0.5082665 , 0.4917335 ],\n",
       "        [0.50374013, 0.4962599 ],\n",
       "        [0.48749906, 0.51250094],\n",
       "        [0.47991067, 0.5200893 ],\n",
       "        [0.5549645 , 0.44503555],\n",
       "        [0.6676359 , 0.3323641 ],\n",
       "        [0.6610429 , 0.33895707],\n",
       "        [0.78479654, 0.21520345],\n",
       "        [0.681203  , 0.31879696],\n",
       "        [0.8044992 , 0.19550085],\n",
       "        [0.941165  , 0.05883504],\n",
       "        [0.61138594, 0.38861406],\n",
       "        [0.4406054 , 0.55939466],\n",
       "        [0.34252793, 0.6574721 ],\n",
       "        [0.43103012, 0.5689699 ],\n",
       "        [0.09014165, 0.90985835],\n",
       "        [0.08027362, 0.9197264 ],\n",
       "        [0.07948285, 0.9205171 ],\n",
       "        [0.4189789 , 0.5810211 ],\n",
       "        [0.53499025, 0.46500972],\n",
       "        [0.6517109 , 0.34828904],\n",
       "        [0.4484176 , 0.55158234],\n",
       "        [0.68452597, 0.315474  ],\n",
       "        [0.21561997, 0.78438   ],\n",
       "        [0.7387274 , 0.26127258],\n",
       "        [0.7726869 , 0.22731313],\n",
       "        [0.886989  , 0.11301096],\n",
       "        [0.889519  , 0.11048099],\n",
       "        [0.81198907, 0.18801092],\n",
       "        [0.91540796, 0.08459207],\n",
       "        [0.89982116, 0.10017884],\n",
       "        [0.69757307, 0.30242696],\n",
       "        [0.8488621 , 0.15113786],\n",
       "        [0.9662058 , 0.03379422],\n",
       "        [0.72956234, 0.27043766],\n",
       "        [0.9005883 , 0.09941168],\n",
       "        [0.9470994 , 0.05290058],\n",
       "        [0.81366247, 0.18633747],\n",
       "        [0.39895952, 0.6010405 ],\n",
       "        [0.17164502, 0.828355  ],\n",
       "        [0.18267173, 0.8173283 ],\n",
       "        [0.15103807, 0.8489619 ],\n",
       "        [0.07373128, 0.9262687 ],\n",
       "        [0.04883633, 0.95116365],\n",
       "        [0.04416329, 0.95583665],\n",
       "        [0.04509174, 0.9549082 ],\n",
       "        [0.02910889, 0.9708911 ],\n",
       "        [0.02169753, 0.9783025 ],\n",
       "        [0.02142825, 0.9785718 ],\n",
       "        [0.02754403, 0.9724559 ],\n",
       "        [0.03722454, 0.96277547],\n",
       "        [0.04530733, 0.9546927 ],\n",
       "        [0.06845969, 0.9315403 ],\n",
       "        [0.14276634, 0.85723364],\n",
       "        [0.28394866, 0.7160514 ],\n",
       "        [0.4563533 , 0.5436467 ],\n",
       "        [0.5149179 , 0.48508206],\n",
       "        [0.59642935, 0.4035707 ],\n",
       "        [0.64173377, 0.35826626],\n",
       "        [0.68164843, 0.31835163],\n",
       "        [0.7435848 , 0.2564152 ],\n",
       "        [0.74545   , 0.25454998],\n",
       "        [0.6176109 , 0.3823891 ],\n",
       "        [0.4834617 , 0.51653826],\n",
       "        [0.633566  , 0.366434  ],\n",
       "        [0.68736154, 0.31263846],\n",
       "        [0.76198846, 0.23801157],\n",
       "        [0.6302911 , 0.3697089 ],\n",
       "        [0.7069267 , 0.2930733 ],\n",
       "        [0.77660394, 0.22339606],\n",
       "        [0.55582887, 0.44417113],\n",
       "        [0.7657493 , 0.2342507 ],\n",
       "        [0.46953148, 0.5304686 ],\n",
       "        [0.678878  , 0.32112193],\n",
       "        [0.59735566, 0.4026443 ],\n",
       "        [0.622136  , 0.37786397],\n",
       "        [0.47616187, 0.5238381 ],\n",
       "        [0.6493228 , 0.35067722],\n",
       "        [0.55977446, 0.44022548],\n",
       "        [0.6968345 , 0.30316547],\n",
       "        [0.724426  , 0.275574  ],\n",
       "        [0.6819136 , 0.3180864 ],\n",
       "        [0.5216632 , 0.47833684],\n",
       "        [0.4939761 , 0.50602394],\n",
       "        [0.5638612 , 0.43613884],\n",
       "        [0.20102695, 0.798973  ],\n",
       "        [0.03937479, 0.96062523],\n",
       "        [0.0342848 , 0.9657152 ],\n",
       "        [0.02471272, 0.9752873 ],\n",
       "        [0.02319479, 0.9768052 ],\n",
       "        [0.0678833 , 0.93211675],\n",
       "        [0.07593493, 0.92406505],\n",
       "        [0.22889614, 0.7711039 ],\n",
       "        [0.60484505, 0.39515495],\n",
       "        [0.6446015 , 0.35539848],\n",
       "        [0.65170515, 0.34829488],\n",
       "        [0.6180857 , 0.38191432],\n",
       "        [0.7317169 , 0.26828313],\n",
       "        [0.58687305, 0.41312695],\n",
       "        [0.47667563, 0.5233244 ],\n",
       "        [0.2597152 , 0.7402848 ],\n",
       "        [0.7491712 , 0.25082883],\n",
       "        [0.8998416 , 0.10015834],\n",
       "        [0.9341857 , 0.06581433],\n",
       "        [0.87948906, 0.12051094],\n",
       "        [0.43777782, 0.5622222 ],\n",
       "        [0.9798978 , 0.02010226],\n",
       "        [0.9849582 , 0.01504178],\n",
       "        [0.9403328 , 0.05966727],\n",
       "        [0.9091713 , 0.09082866],\n",
       "        [0.5507578 , 0.44924217],\n",
       "        [0.92741305, 0.07258695],\n",
       "        [0.7600319 , 0.23996809],\n",
       "        [0.8698148 , 0.13018522],\n",
       "        [0.67173266, 0.3282674 ],\n",
       "        [0.3049659 , 0.6950341 ],\n",
       "        [0.14194989, 0.85805017],\n",
       "        [0.06019249, 0.9398075 ],\n",
       "        [0.03106552, 0.9689344 ],\n",
       "        [0.02052192, 0.9794781 ],\n",
       "        [0.01790397, 0.9820961 ],\n",
       "        [0.01726004, 0.9827399 ],\n",
       "        [0.01991157, 0.9800884 ],\n",
       "        [0.03179635, 0.9682036 ],\n",
       "        [0.07367244, 0.92632765],\n",
       "        [0.18619776, 0.8138022 ],\n",
       "        [0.33426827, 0.66573167],\n",
       "        [0.43541977, 0.56458026],\n",
       "        [0.52543765, 0.47456235],\n",
       "        [0.567524  , 0.432476  ],\n",
       "        [0.52565795, 0.47434205],\n",
       "        [0.4585861 , 0.54141396],\n",
       "        [0.43414128, 0.5658587 ],\n",
       "        [0.5494814 , 0.4505186 ],\n",
       "        [0.612986  , 0.38701394],\n",
       "        [0.62558264, 0.3744173 ],\n",
       "        [0.51131   , 0.48869005],\n",
       "        [0.5531703 , 0.4468297 ],\n",
       "        [0.696543  , 0.30345702],\n",
       "        [0.48291758, 0.5170824 ],\n",
       "        [0.39006212, 0.6099379 ],\n",
       "        [0.8425582 , 0.1574418 ],\n",
       "        [0.8782276 , 0.12177236],\n",
       "        [0.7200885 , 0.27991152],\n",
       "        [0.9383522 , 0.06164781],\n",
       "        [0.9265931 , 0.07340682],\n",
       "        [0.8018095 , 0.19819047],\n",
       "        [0.5590196 , 0.44098035],\n",
       "        [0.2616517 , 0.73834825],\n",
       "        [0.25543374, 0.74456626],\n",
       "        [0.09330964, 0.9066904 ],\n",
       "        [0.05146199, 0.94853795],\n",
       "        [0.02935561, 0.9706444 ],\n",
       "        [0.03714586, 0.96285415],\n",
       "        [0.07148957, 0.9285104 ],\n",
       "        [0.16223328, 0.83776665],\n",
       "        [0.28396043, 0.71603954],\n",
       "        [0.37558973, 0.6244103 ],\n",
       "        [0.43600345, 0.56399655],\n",
       "        [0.45706668, 0.54293334],\n",
       "        [0.46932885, 0.5306712 ],\n",
       "        [0.45481586, 0.5451841 ],\n",
       "        [0.43481207, 0.56518793],\n",
       "        [0.4064668 , 0.59353316],\n",
       "        [0.39375928, 0.6062407 ],\n",
       "        [0.41079476, 0.5892052 ],\n",
       "        [0.40494078, 0.5950592 ],\n",
       "        [0.42287642, 0.5771236 ]], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.92156671667099]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_r.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fd40e00383c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# see what's in model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lstm_r.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "# see what's in model\n",
    "model.load('lstm_r.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from keras.models import load_model, Model\n",
    "from sa import fetch_dsa, fetch_lsa, get_sc, \n",
    "from utils import *\n",
    "\n",
    "model = load_model('lstm_r.h5')\n",
    "default_upper_bound = 2000\n",
    "default_n_bucket = 1000\n",
    "default_n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(): #创建一个类\n",
    "    def __init__(self): #定义初始化信息。\n",
    "        self.is_classification = True\n",
    "        self.save_path = './tmp/'\n",
    "        self.d = 'lstm_r'\n",
    "        self.num_classes = 2\n",
    "        self.lsa = True\n",
    "        self.dsa = True\n",
    "        self.target = 'none'\n",
    "        self.batch_size = 128\n",
    "        self.var_threshold = 1e-5\n",
    "        self.upper_bound = 2000\n",
    "        self.n_bucket = 1000\n",
    "        self.num_classes = 10\n",
    "        self.is_classification = True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_lsa(model, x_train, x_target, target_name, layer_names, args):\n",
    "    \"\"\"Likelihood-based SA\n",
    "\n",
    "    Args:\n",
    "        model (keras model): Subject model.\n",
    "        x_train (list): Set of training inputs.\n",
    "        x_target (list): Set of target (test or[] adversarial) inputs.\n",
    "        target_name (str): Name of target set.\n",
    "        layer_names (list): List of selected layer names.\n",
    "        args: Keyboard args.\n",
    "\n",
    "    Returns:\n",
    "        lsa (list): List of lsa for each target input.\n",
    "    \"\"\"\n",
    "\n",
    "    prefix = info(\"[\" + target_name + \"] \")\n",
    "    train_ats, train_pred, target_ats, target_pred = _get_train_target_ats(\n",
    "        model, x_train, x_target, target_name, layer_names, args\n",
    "    )\n",
    "\n",
    "    class_matrix = {}\n",
    "    if args.is_classification:\n",
    "        for i, label in enumerate(train_pred):\n",
    "            label = label[0]\n",
    "            if label not in class_matrix:\n",
    "                class_matrix[label] = []\n",
    "            class_matrix[label].append(i)\n",
    "\n",
    "    kdes, removed_cols = _get_kdes(train_ats, train_pred, class_matrix, args)\n",
    "\n",
    "    lsa = []\n",
    "    print(prefix + \"Fetching LSA\")\n",
    "    if args.is_classification:\n",
    "        for i, at in enumerate(tqdm(target_ats)):\n",
    "            label = target_pred[i]\n",
    "            kde = kdes[label]\n",
    "            lsa.append(_get_lsa(kde, at, removed_cols))\n",
    "    else:\n",
    "        kde = kdes[0]\n",
    "        for at in tqdm(target_ats):\n",
    "            lsa.append(_get_lsa(kde, at, removed_cols))\n",
    "\n",
    "    return lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['lstm_34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.predict_classes(x_test)\n",
    "# matrix = {}\n",
    "# for i,label in enumerate(pred):\n",
    "#     if label[0] not in matrix:\n",
    "#         print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lsa = fetch_lsa(model, x_train, x_test, \"test\", layer_names, args)\n",
    "target_lsa = fetch_lsa(model, x_train, x_test, \"target\", layer_names, args)\n",
    "# target_lsa = fetch_lsa(model, x_train, x_target, args.target, layer_names, args)\n",
    "target_cov = get_sc(\n",
    "    np.amin(target_lsa), default_upper_bound, default_n_bucket, target_lsa\n",
    ")\n",
    "\n",
    "auc = compute_roc_auc(test_lsa, target_lsa)\n",
    "print(infog(\"ROC-AUC: \" + str(auc * 100)))\n",
    "\n",
    "print(infog(\"LSA coverage: \" + str(target_cov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dsa = fetch_dsa(model, x_train, x_test, \"test\", layer_names, args)\n",
    "\n",
    "target_dsa = fetch_dsa(model, x_train, x_test, args.target, layer_names, args)\n",
    "target_cov = get_sc(\n",
    "    np.amin(target_dsa), default_upper_bound, default_n_bucket, target_dsa\n",
    ")\n",
    "\n",
    "auc = compute_roc_auc(test_dsa, target_dsa)\n",
    "print(infog(\"ROC-AUC: \" + str(auc * 100)))\n",
    "\n",
    "print(infog(\"DSA coverage: \" + str(target_cov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
